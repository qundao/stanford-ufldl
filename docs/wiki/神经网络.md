神经网络
====

<!-- Jump to: [navigation](#column-one), [search](#searchInput) -->

|  |
| --- |
| Contents* [1 概述](#.E6.A6.82.E8.BF.B0)
* [2 神经网络模型](#.E7.A5.9E.E7.BB.8F.E7.BD.91.E7.BB.9C.E6.A8.A1.E5.9E.8B)
* [3 中英文对照](#.E4.B8.AD.E8.8B.B1.E6.96.87.E5.AF.B9.E7.85.A7)
* [4 中文译者](#.E4.B8.AD.E6.96.87.E8.AF.91.E8.80.85)
 |

 概述
---

以监督学习为例，假设我们有训练样本集 ![\textstyle (x(^ i),y(^ i))](images/math/7/0/e/70ebbf3d401302b5d148530b986f0602.png) ，那么神经网络算法能够提供一种复杂且非线性的假设模型 ![\textstyle h_{W,b}(x)](images/math/5/8/d/58d3a4fe4ad68b333b180071dd46db82.png) ，它具有参数 ![\textstyle W, b](images/math/7/c/9/7c9aa03f5258ecf79556ba374d7eb2cd.png) ，可以以此参数来拟合我们的数据。

为了描述神经网络，我们先从最简单的神经网络讲起，这个神经网络仅由一个“神经元”构成，以下即是这个“神经元”的图示：

![SingleNeuron.png](images/thumb/3/3d/SingleNeuron.png/300px-SingleNeuron.png)

这个“神经元”是一个以 ![\textstyle x_1, x_2, x_3](images/math/3/c/b/3cb2ab026a8bb3279a30485c2220a5a4.png) 及截距 ![\textstyle +1](images/math/d/c/b/dcb8dd3d14a2c0aa9b06ec6ce4ec0d59.png) 为输入值的运算单元，其输出为 ![\textstyle  h_{W,b}(x) = f(W^Tx) = f(\sum_{i=1}^3 W_{i}x_i +b)](images/math/8/9/f/89f1f9e549b908834d9fedca36d07bd4.png) ，其中函数 ![\textstyle f : \Re \mapsto \Re](images/math/5/d/f/5df2a707a6b2421afcb345f96051297e.png) 被称为“激活函数”。在本教程中，我们选用sigmoid函数作为**激活函数** ![\textstyle f(\cdot)](images/math/0/3/0/0303dd697c0e1b72185d7939f9870784.png)

![
f(z) = \frac{1}{1+\exp(-z)}.
](images/math/c/e/5/ce5df10952ab30aa868f44db2f77486b.png)

可以看出，这个单一“神经元”的输入－输出映射关系其实就是一个逻辑回归（logistic regression）。

虽然本系列教程采用sigmoid函数，但你也可以选择双曲正切函数（tanh）：

![
f(z) = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}},  
](images/math/a/9/0/a9025d0884453bd5898c9681e871b3fb.png)

以下分别是sigmoid及tanh的函数图像

![Sigmoid activation function.](images/thumb/c/ca/Sigmoid_Function.png/400px-Sigmoid_Function.png)
![Tanh activation function.](images/thumb/a/aa/Tanh_Function.png/400px-Tanh_Function.png)

![\textstyle \tanh(z)](images/math/8/7/e/87e9b5fc0869fae518eed4b75536334f.png) 函数是sigmoid函数的一种变体，它的取值范围为 ![\textstyle [-1,1]](images/math/8/5/a/85a1c5a07f21a9eebbfb1dca380f8d38.png) ，而不是sigmoid函数的 ![\textstyle [0,1]](images/math/8/4/2/84235d31ac83fe764546463aba7acc0e.png) 。

注意，与其它地方（包括OpenClassroom公开课以及斯坦福大学CS229课程）不同的是，这里我们不再令 ![\textstyle x_0=1](images/math/c/5/8/c582053ce9cb63d69ae80acb53ded0d3.png) 。取而代之，我们用单独的参数 ![\textstyle b](images/math/5/2/5/5254b90d248051980262672a1bbc2433.png) 来表示截距。

最后要说明的是，有一个等式我们以后会经常用到：如果选择 ![\textstyle f(z) = 1/(1+\exp(-z))](images/math/e/c/6/ec62a4df6800f8c9ea680a08003df5c3.png) ，也就是sigmoid函数，那么它的导数就是 ![\textstyle f'(z) = f(z) (1-f(z))](images/math/9/9/4/994ac235e9478c8f465a4acdd8aae017.png) （如果选择tanh函数，那它的导数就是 ![\textstyle f'(z) = 1- (f(z))^2](images/math/e/7/d/e7deb0493f3858b59b86181afe368fec.png) ，你可以根据sigmoid（或tanh）函数的定义自行推导这个等式。

 神经网络模型
-------

所谓神经网络就是将许多个单一“神经元”联结在一起，这样，一个“神经元”的输出就可以是另一个“神经元”的输入。例如，下图就是一个简单的神经网络：

![Network331.png](images/thumb/9/99/Network331.png/400px-Network331.png)
我们使用圆圈来表示神经网络的输入，标上“![\textstyle +1](images/math/d/c/b/dcb8dd3d14a2c0aa9b06ec6ce4ec0d59.png)”的圆圈被称为**偏置节点**，也就是截距项。神经网络最左边的一层叫做**输入层**，最右的一层叫做**输出层**（本例中，输出层只有一个节点）。中间所有节点组成的一层叫做**隐藏层**，因为我们不能在训练样本集中观测到它们的值。同时可以看到，以上神经网络的例子中有3个**输入单元**（偏置单元不计在内），3个**隐藏单元**及一个**输出单元**。

我们用 ![\textstyle {n}_l](images/math/5/4/6/546158a6d0082614d47e7f8a63225b0b.png) 来表示网络的层数，本例中 ![\textstyle n_l=3](images/math/3/c/8/3c89b5db1e49221343428af57c90e44a.png) ，我们将第 ![\textstyle l](images/math/b/a/0/ba0593b3db2fa8535b077516f4b0d70b.png) 层记为 ![\textstyle L_l](images/math/5/5/e/55ea36127aa64b92b071c269cd1e3990.png) ，于是 ![\textstyle L_1](images/math/1/3/e/13e0887b9e716279d9a7b8bc8e6ad63b.png) 是输入层，输出层是 ![\textstyle L_{n_l}](images/math/2/2/1/221a7296664022427d488fdb9b14b19b.png) 。本例神经网络有参数 ![\textstyle (W,b) = (W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)})](images/math/a/a/3/aa3d6ed3c577d41a791324008558efbe.png) ，其中 ![\textstyle W^{(l)}_{ij}](images/math/d/f/e/dfe43c64e3c42ea4ff1774fc82b87805.png) （下面的式子中用到）是第 ![\textstyle l](images/math/b/a/0/ba0593b3db2fa8535b077516f4b0d70b.png) 层第 ![\textstyle j](images/math/2/3/5/235c5146ab110558897640c34dad7d97.png) 单元与第 ![\textstyle l+1](images/math/9/0/6/9068105ec8ebb97277c937bfa61b606d.png) 层第 ![\textstyle i](images/math/0/b/3/0b36ee693126b34b58f77dba7ed23987.png) 单元之间的联接参数（其实就是连接线上的权重，注意标号顺序）， ![\textstyle b^{(l)}_i](images/math/4/c/7/4c786c16575b63bbb554254725b6b648.png) 是第 ![\textstyle l+1](images/math/9/0/6/9068105ec8ebb97277c937bfa61b606d.png) 层第 ![\textstyle i](images/math/0/b/3/0b36ee693126b34b58f77dba7ed23987.png) 单元的偏置项。因此在本例中， ![\textstyle W^{(1)} \in \Re^{3\times 3}](images/math/5/c/a/5ca0efbb17e86cb00091f6a528e0ab0e.png) ， ![\textstyle W^{(2)} \in \Re^{1\times 3}](images/math/4/3/1/431cf6f298e4106efb5bff4495aa3c6d.png) 。注意，没有其他单元连向偏置单元(即偏置单元没有输入)，因为它们总是输出 ![\textstyle +1](images/math/d/c/b/dcb8dd3d14a2c0aa9b06ec6ce4ec0d59.png)。同时，我们用 ![\textstyle s_l](images/math/8/a/f/8afb62ac69ccb2911bb24795ff052a07.png) 表示第 ![\textstyle l](images/math/b/a/0/ba0593b3db2fa8535b077516f4b0d70b.png) 层的节点数（偏置单元不计在内）。

我们用 ![\textstyle a^{(l)}_i](images/math/c/9/b/c9b144e0a6735fafb01b3615a2a0dc05.png) 表示第 ![\textstyle l](images/math/b/a/0/ba0593b3db2fa8535b077516f4b0d70b.png) 层第 ![\textstyle i](images/math/0/b/3/0b36ee693126b34b58f77dba7ed23987.png) 单元的**激活值**（输出值）。当 ![\textstyle l=1](images/math/4/a/4/4a4725e295806f22b26342fe3cd3338f.png) 时， ![\textstyle a^{(1)}_i = x_i](images/math/f/5/c/f5c1979e94318aee674de68348b96557.png) ，也就是第 ![\textstyle i](images/math/0/b/3/0b36ee693126b34b58f77dba7ed23987.png) 个输入值（输入值的第 ![\textstyle i](images/math/0/b/3/0b36ee693126b34b58f77dba7ed23987.png) 个特征）。对于给定参数集合 ![\textstyle W,b](images/math/7/c/9/7c9aa03f5258ecf79556ba374d7eb2cd.png) ，我们的神经网络就可以按照函数 ![\textstyle h_{W,b}(x)](images/math/5/8/d/58d3a4fe4ad68b333b180071dd46db82.png) 来计算输出结果。本例神经网络的计算步骤如下：

![ 
\begin{align}
a_1^{(2)} &= f(W_{11}^{(1)}x_1 + W_{12}^{(1)} x_2 + W_{13}^{(1)} x_3 + b_1^{(1)})  \\
a_2^{(2)} &= f(W_{21}^{(1)}x_1 + W_{22}^{(1)} x_2 + W_{23}^{(1)} x_3 + b_2^{(1)})  \\
a_3^{(2)} &= f(W_{31}^{(1)}x_1 + W_{32}^{(1)} x_2 + W_{33}^{(1)} x_3 + b_3^{(1)})  \\
h_{W,b}(x) &= a_1^{(3)} =  f(W_{11}^{(2)}a_1^{(2)} + W_{12}^{(2)} a_2^{(2)} + W_{13}^{(2)} a_3^{(2)} + b_1^{(2)}) 
\end{align}
](images/math/f/d/e/fde22a388f607f526f03644c71a72f92.png)

我们用 ![\textstyle z^{(l)}_i](images/math/3/d/d/3dd5c56e0949e76de86690e1b868cdcf.png) 表示第 ![\textstyle l](images/math/b/a/0/ba0593b3db2fa8535b077516f4b0d70b.png) 层第 ![\textstyle i](images/math/0/b/3/0b36ee693126b34b58f77dba7ed23987.png) 单元输入加权和（包括偏置单元），比如， ![\textstyle  z_i^{(2)} = \sum_{j=1}^n W^{(1)}_{ij} x_j + b^{(1)}_i](images/math/a/a/e/aae7340fe1eb75c824b8abc107c3db27.png) ，则 ![\textstyle a^{(l)}_i = f(z^{(l)}_i)](images/math/c/f/8/cf8cb56750f5aaca7dc59480a53d9676.png) 。

这样我们就可以得到一种更简洁的表示法。这里我们将激活函数 ![\textstyle f(\cdot)](images/math/0/3/0/0303dd697c0e1b72185d7939f9870784.png) 扩展为用向量（分量的形式）来表示，即 ![\textstyle f([z_1, z_2, z_3]) = [f(z_1), f(z_2), f(z_3)]](images/math/d/b/8/db84346dcd6187f0fbb0f6c1a72eecf8.png) ，那么，上面的等式可以更简洁地表示为：

![\begin{align}
z^{(2)} &= W^{(1)} x + b^{(1)} \\
a^{(2)} &= f(z^{(2)}) \\
z^{(3)} &= W^{(2)} a^{(2)} + b^{(2)} \\
h_{W,b}(x) &= a^{(3)} = f(z^{(3)})
\end{align}](images/math/9/6/9/9690acc03c1e5133b0509257b532b4f7.png)

我们将上面的计算步骤叫作**前向传播**。回想一下，之前我们用 ![\textstyle a^{(1)} = x](images/math/d/e/0/de0b51a7e4a2b2047d52a165419ac048.png) 表示输入层的激活值，那么给定第 ![\textstyle l](images/math/b/a/0/ba0593b3db2fa8535b077516f4b0d70b.png) 层的激活值 ![\textstyle a^{(l)}](images/math/b/d/2/bd2728b5337ccec5b5729756d5796b20.png) 后，第 ![\textstyle l+1](images/math/9/0/6/9068105ec8ebb97277c937bfa61b606d.png) 层的激活值 ![\textstyle a^{(l+1)}](images/math/e/b/8/eb8a863a7b57397bf06a0532d4f1daf1.png) 就可以按照下面步骤计算得到：

![ \begin{align}
z^{(l+1)} &= W^{(l)} a^{(l)} + b^{(l)}   \\
a^{(l+1)} &= f(z^{(l+1)})
\end{align}](images/math/5/c/f/5cfcbbe6d55b6c882f56a85a57eafe6e.png)

将参数矩阵化，使用矩阵－向量运算方式，我们就可以利用线性代数的优势对神经网络进行快速求解。

目前为止，我们讨论了一种神经网络，我们也可以构建另一种**结构**的神经网络（这里结构指的是神经元之间的联接模式），也就是包含多个隐藏层的神经网络。最常见的一个例子是 ![\textstyle  n_l](images/math/5/b/7/5b7a0657fdea25f29866c8e1d6e884ac.png) 层的神经网络，第 ![\textstyle  1](images/math/6/e/9/6e924e04b5c9d4c5be131609a038b821.png) 层是输入层，第 ![\textstyle  n_l](images/math/5/b/7/5b7a0657fdea25f29866c8e1d6e884ac.png) 层是输出层，中间的每个层 ![\textstyle  l](images/math/b/a/0/ba0593b3db2fa8535b077516f4b0d70b.png) 与层 ![\textstyle  l+1](images/math/9/0/6/9068105ec8ebb97277c937bfa61b606d.png) 紧密相联。这种模式下，要计算神经网络的输出结果，我们可以按照之前描述的等式，按部就班，进行前向传播，逐一计算第 ![\textstyle  L_2](images/math/c/f/7/cf7d186efd913f4fb9ceb939bf5135c4.png) 层的所有激活值，然后是第 ![\textstyle  L_3](images/math/d/9/b/d9b949d768ca8bab18830d9efc3fa441.png) 层的激活值，以此类推，直到第 ![\textstyle  L_{n_l}](images/math/2/2/1/221a7296664022427d488fdb9b14b19b.png) 层。这是一个**前馈**神经网络的例子，因为这种联接图没有闭环或回路。

神经网络也可以有多个输出单元。比如，下面的神经网络有两层隐藏层： ![\textstyle L_2](images/math/c/f/7/cf7d186efd913f4fb9ceb939bf5135c4.png) 及 ![\textstyle L_3](images/math/d/9/b/d9b949d768ca8bab18830d9efc3fa441.png) ，输出层 ![\textstyle L_4](images/math/a/b/0/ab05e0667abe37f2e3cbc05735573034.png) 有两个输出单元。

![Network3322.png](images/thumb/4/40/Network3322.png/500px-Network3322.png)

要求解这样的神经网络，需要样本集 ![\textstyle (x^{(i)}, y^{(i)})](images/math/f/1/7/f178249571382c3921d2c46f7abd47da.png) ，其中 ![\textstyle y^{(i)} \in \Re^2](images/math/9/e/d/9edce3bff2898e4b7f084ad3a2bbf494.png) 。如果你想预测的输出是多个的，那这种神经网络很适用。（比如，在医疗诊断应用中，患者的体征指标就可以作为向量的输入值，而不同的输出值 ![\textstyle y_i](images/math/7/a/5/7a5d164f3df0329a8032cda67d95d9d4.png) 可以表示不同的疾病存在与否。）

 中英文对照
------

neural networks 神经网络

activation function 激活函数

hyperbolic tangent 双曲正切函数

bias units 偏置项

activation 激活值

forward propagation 前向传播

feedforward neural network 前馈神经网络(参照Mitchell的《机器学习》的翻译)

 中文译者
-----

孙逊（sunpaofu@foxmail.com），林锋（xlfg@yeah.net），刘鸿鹏飞（just.dark@foxmail.com）, 许利杰（csxulijie@gmail.com）

**神经网络** | [反向传导算法](%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95.md "反向传导算法") | [梯度检验与高级优化](%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%E4%B8%8E%E9%AB%98%E7%BA%A7%E4%BC%98%E5%8C%96.md "梯度检验与高级优化") | [自编码算法与稀疏性](%E8%87%AA%E7%BC%96%E7%A0%81%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%80%E7%96%8F%E6%80%A7.md "自编码算法与稀疏性") | [可视化自编码器训练结果](%E5%8F%AF%E8%A7%86%E5%8C%96%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C.md "可视化自编码器训练结果") | [稀疏自编码器符号一览表](%E7%A8%80%E7%96%8F%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E7%AC%A6%E5%8F%B7%E4%B8%80%E8%A7%88%E8%A1%A8.md "稀疏自编码器符号一览表") | [Exercise:Sparse\_Autoencoder](Exercise_Sparse_Autoencoder.md "Exercise:Sparse Autoencoder")

---

> * Language: [English](Neural_Networks.md "Neural Networks")
> * This page was last modified on 7 April 2013, at 12:34.

