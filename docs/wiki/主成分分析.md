主成分分析
=====

<!-- Jump to: [navigation](#column-one), [search](#searchInput) -->

|  |
| --- |
| Contents* [1 引言](#.E5.BC.95.E8.A8.80)
* [2 实例和数学背景](#.E5.AE.9E.E4.BE.8B.E5.92.8C.E6.95.B0.E5.AD.A6.E8.83.8C.E6.99.AF)
* [3 旋转数据](#.E6.97.8B.E8.BD.AC.E6.95.B0.E6.8D.AE)
* [4 数据降维](#.E6.95.B0.E6.8D.AE.E9.99.8D.E7.BB.B4)
* [5 还原近似数据](#.E8.BF.98.E5.8E.9F.E8.BF.91.E4.BC.BC.E6.95.B0.E6.8D.AE)
* [6 选择主成分个数](#.E9.80.89.E6.8B.A9.E4.B8.BB.E6.88.90.E5.88.86.E4.B8.AA.E6.95.B0)
* [7 对图像数据应用PCA算法](#.E5.AF.B9.E5.9B.BE.E5.83.8F.E6.95.B0.E6.8D.AE.E5.BA.94.E7.94.A8PCA.E7.AE.97.E6.B3.95)
* [8 参考文献](#.E5.8F.82.E8.80.83.E6.96.87.E7.8C.AE)
* [9 中英文对照](#.E4.B8.AD.E8.8B.B1.E6.96.87.E5.AF.B9.E7.85.A7)
* [10 中文译者](#.E4.B8.AD.E6.96.87.E8.AF.91.E8.80.85)
 |

  引言
----

主成分分析（PCA）是一种能够极大提升无监督特征学习速度的数据降维算法。更重要的是，理解PCA算法，对实现**白化**算法有很大的帮助，很多算法都先用白化算法作预处理步骤。

假设你使用图像来训练算法，因为图像中相邻的像素高度相关，输入数据是有一定冗余的。具体来说，假如我们正在训练的16x16灰度值图像，记为一个256维向量 ![\textstyle x \in \Re^{256}](images/math/3/e/c/3ec732c534e730334fbe728ae49c8fce.png) ，其中特征值 ![\textstyle x_j](images/math/b/d/f/bdf5b20642553027712d5b5240b31cf3.png) 对应每个像素的亮度值。由于相邻像素间的相关性，PCA算法可以将输入向量转换为一个维数低很多的近似向量，而且误差非常小。

  实例和数学背景
---------

在我们的实例中，使用的输入数据集表示为 ![\textstyle \{x^{(1)}, x^{(2)}, \ldots, x^{(m)}\}](images/math/b/b/f/bbfa674fd83f37c2c66867d7e0cc264a.png) ，维度 ![\textstyle n=2](images/math/b/1/9/b1993eef97e184af6b11db01e694445f.png) 即 ![\textstyle x^{(i)} \in \Re^2](images/math/1/b/a/1babb19c8b06f9a7bd624fa60f29d5fb.png) 。假设我们想把数据从2维降到1维。（在实际应用中，我们也许需要把数据从256维降到50维；在这里使用低维数据，主要是为了更好地可视化算法的行为）。下图是我们的数据集：

![PCA-rawdata.png](images/thumb/b/ba/PCA-rawdata.png/600px-PCA-rawdata.png)

这些数据已经进行了预处理，使得每个特征 ![\textstyle x_1](images/math/f/a/7/fa7eebd32aa8c9cdae2b2aacbc324331.png) 和 ![\textstyle x_2](images/math/7/6/8/76879b7da23d4991dfcb03323403c152.png) 具有相同的均值（零）和方差。

为方便展示，根据 ![\textstyle x_1](images/math/f/a/7/fa7eebd32aa8c9cdae2b2aacbc324331.png) 值的大小，我们将每个点分别涂上了三种颜色之一，但该颜色并不用于算法而仅用于图解。

PCA算法将寻找一个低维空间来投影我们的数据。从下图中可以看出， ![\textstyle u_1](images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png) 是数据变化的主方向，而 ![\textstyle u_2](images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png) 是次方向。

![PCA-u1.png](images/thumb/b/b4/PCA-u1.png/600px-PCA-u1.png)

也就是说，数据在 ![\textstyle u_1](images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png) 方向上的变化要比在 ![\textstyle u_2](images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png) 方向上大。为更形式化地找出方向 ![\textstyle u_1](images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png) 和 ![\textstyle u_2](images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png) ，我们首先计算出矩阵 ![\textstyle \Sigma](images/math/6/6/9/669ec82a71dede49eb73e539bc3423b6.png) ，如下所示：

![\begin{align}
\Sigma = \frac{1}{m} \sum_{i=1}^m (x^{(i)})(x^{(i)})^T. 
\end{align}](images/math/d/a/9/da9b50ec05dbe4ae513e4f52093b8342.png)

假设 ![\textstyle x](images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 的均值为零，那么 ![\textstyle \Sigma](images/math/6/6/9/669ec82a71dede49eb73e539bc3423b6.png) 就是x的协方差矩阵。（符号 ![\textstyle \Sigma](images/math/6/6/9/669ec82a71dede49eb73e539bc3423b6.png) ，读"Sigma"，是协方差矩阵的标准符号。虽然看起来与求和符号 ![\sum_{i=1}^n i](images/math/7/3/b/73b577d2b026ab8f8fb733953266427e.png) 比较像，但它们其实是两个不同的概念。）

可以证明，数据变化的主方向 ![\textstyle u_1](images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png) 就是协方差矩阵 ![\textstyle \Sigma](images/math/6/6/9/669ec82a71dede49eb73e539bc3423b6.png) 的主特征向量，而 ![\textstyle u_2](images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png) 是次特征向量。

注：如果你对如何得到这个结果的具体数学推导过程感兴趣，可以参看CS229（机器学习）PCA部分的课件（链接在本页底部）。但如果仅仅是想跟上本课，可以不必如此。

你可以通过标准的数值线性代数运算软件求得特征向量（见实现说明）.我们先计算出协方差矩阵![\textstyle \Sigma](images/math/6/6/9/669ec82a71dede49eb73e539bc3423b6.png)的特征向量，按列排放，而组成矩阵![\textstyle U](images/math/6/a/5/6a55fb16b0464ccd6652a7f2a583217f.png)：

![\begin{align}
U = 
\begin{bmatrix} 
| & | & & |  \\
u_1 & u_2 & \cdots & u_n  \\
| & | & & | 
\end{bmatrix} 		
\end{align}](images/math/6/9/0/6906da1c5ac5f7f94a3b337447e69360.png)

此处， ![\textstyle u_1](images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png) 是主特征向量（对应最大的特征值）， ![\textstyle u_2](images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png) 是次特征向量。以此类推，另记 ![\textstyle \lambda_1, \lambda_2, \ldots, \lambda_n](images/math/d/2/b/d2b02582947d98e3be81be3d1e684f28.png) 为相应的特征值。

在本例中，向量 ![\textstyle u_1](images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png) 和 ![\textstyle u_2](images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png) 构成了一个新基，可以用来表示数据。令 ![\textstyle x \in \Re^2](images/math/b/2/6/b260df225bb49f3ff776b17a50cd20d3.png) 为训练样本，那么 ![\textstyle u_1^Tx](images/math/7/c/0/7c0e7fb10fb6e75bad211b2f2070c24c.png) 就是样本点 ![\textstyle x](images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 在维度 ![\textstyle u_1](images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png) 上的投影的长度（幅值）。同样的， ![\textstyle u_2^Tx](images/math/3/8/9/389b689de5736f95b05c3be9c373b95a.png) 是 ![\textstyle x](images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 投影到 ![\textstyle u_2](images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png) 维度上的幅值。

  旋转数据
------

至此，我们可以把 ![\textstyle x](images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 用 ![\textstyle (u_1, u_2)](images/math/0/3/2/0329a7ca7eca352beded9f24406d34fe.png) 基表达为：

![\begin{align}
x_{\rm rot} = U^Tx = \begin{bmatrix} u_1^Tx \\ u_2^Tx \end{bmatrix} 
\end{align}](images/math/e/a/a/eaa1e40a68e966dd5a3d272dd6d091ed.png)

（下标“rot”来源于单词“rotation”，意指这是原数据经过旋转（也可以说成映射）后得到的结果）

对数据集中的每个样本 ![\textstyle i](images/math/0/b/3/0b36ee693126b34b58f77dba7ed23987.png) 分别进行旋转： ![\textstyle x_{\rm rot}^{(i)} = U^Tx^{(i)}](images/math/c/d/0/cd047246fd68f6d52b2fd068e063c0ef.png) for every ![\textstyle i](images/math/0/b/3/0b36ee693126b34b58f77dba7ed23987.png) ，然后把变换后的数据 ![\textstyle x_{\rm rot}](images/math/1/7/0/170047e804738636731477291969d554.png) 显示在坐标图上，可得：

![PCA-rotated.png](images/thumb/1/12/PCA-rotated.png/600px-PCA-rotated.png)

这就是把训练数据集旋转到 ![\textstyle u_1](images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png)，![\textstyle u_2](images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png) 基后的结果。一般而言，运算 ![\textstyle U^Tx](images/math/e/0/a/e0aec5d033ea89dc9bd9c83bc2b4edec.png) 表示旋转到基 ![\textstyle u_1](images/math/3/f/c/3fc01c8dc5d4c8c57cd758ec3a76283f.png),![\textstyle u_2](images/math/e/d/9/ed99a7fbd444e14555ad4f8eac78b94b.png), ...,![\textstyle u_n](images/math/0/b/e/0be80bb4e50881840b92fb8331ef2bbd.png) 之上的训练数据。矩阵 ![\textstyle U](images/math/6/a/5/6a55fb16b0464ccd6652a7f2a583217f.png) 有正交性，即满足 ![\textstyle U^TU = UU^T = I](images/math/a/8/2/a825fd85c23ffa9b851fb64c9c816ad6.png) ，所以若想将旋转后的向量 ![\textstyle x_{\rm rot}](images/math/1/7/0/170047e804738636731477291969d554.png) 还原为原始数据 ![\textstyle x](images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) ，将其左乘矩阵![\textstyle U](images/math/6/a/5/6a55fb16b0464ccd6652a7f2a583217f.png)即可： ![\textstyle x=U x_{\rm rot}](images/math/f/a/a/faada910e82b90d1c221943616cc85ab.png) , 验算一下： ![\textstyle U x_{\rm rot} =  UU^T x = x](images/math/a/5/f/a5fa6224542f5b2871447986260574d2.png).

  数据降维
------

数据的主方向就是旋转数据的第一维 ![\textstyle x_{{\rm rot},1}](images/math/0/0/6/0066d1e2efa2f0019a3dfd3469862934.png) 。因此，若想把这数据降到一维，可令：

![\begin{align}
\tilde{x}^{(i)} = x_{{\rm rot},1}^{(i)} = u_1^Tx^{(i)} \in \Re.
\end{align}](images/math/8/c/f/8cf51b2f3bf8c78ad1b03c27aa68f692.png)

更一般的，假如想把数据 ![\textstyle x \in \Re^n](images/math/9/e/b/9ebd39996afb169318c1dd5fb1503b17.png) 降到 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 维表示 
![\textstyle \tilde{x} \in \Re^k](images/math/2/1/3/21337248295f42f7fe18d9a9b3da57b1.png) （令 ![\textstyle k < n](images/math/8/7/b/87b6508de7e0487479389cff2b5fa91a.png) ）,只需选取 ![\textstyle x_{\rm rot}](images/math/1/7/0/170047e804738636731477291969d554.png) 的前 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个成分，分别对应前 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个数据变化的主方向。

PCA的另外一种解释是：![\textstyle x_{\rm rot}](images/math/1/7/0/170047e804738636731477291969d554.png) 是一个 ![\textstyle n](images/math/0/c/5/0c59de0fa75c1baa1c024aabfa43b2e3.png) 维向量，其中前几个成分可能比较大（例如，上例中大部分样本第一个成分 ![\textstyle x_{{\rm rot},1}^{(i)} = u_1^Tx^{(i)}](images/math/8/0/e/80ebba0459d97a31a03e9de6b0957c31.png) 的取值相对较大），而后面成分可能会比较小（例如，上例中大部分样本的 ![\textstyle x_{{\rm rot},2}^{(i)} = u_2^Tx^{(i)}](images/math/4/6/8/468a726aaaea7f4aabbeb8a2e1966aae.png) 较小）。

PCA算法做的其实就是丢弃 ![\textstyle x_{\rm rot}](images/math/1/7/0/170047e804738636731477291969d554.png) 中后面（取值较小）的成分，就是将这些成分的值近似为零。具体的说，设 ![\textstyle \tilde{x}](images/math/1/a/6/1a62e33dcf57261829692126a4dcd02d.png) 是 ![\textstyle x_{{\rm rot}}](images/math/7/7/4/774d8fa9b41f58dfc57cebb419e0de60.png) 的近似表示，那么将 ![\textstyle x_{{\rm rot}}](images/math/7/7/4/774d8fa9b41f58dfc57cebb419e0de60.png) 除了前 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个成分外，其余全赋值为零，就得到：

![\begin{align}
\tilde{x} = 
\begin{bmatrix} 
x_{{\rm rot},1} \\
\vdots \\ 
x_{{\rm rot},k} \\
0 \\ 
\vdots \\ 
0 \\ 
\end{bmatrix}
\approx 
\begin{bmatrix} 
x_{{\rm rot},1} \\
\vdots \\ 
x_{{\rm rot},k} \\
x_{{\rm rot},k+1} \\
\vdots \\ 
x_{{\rm rot},n} 
\end{bmatrix}
= x_{\rm rot} 
\end{align}](images/math/5/e/8/5e8f3f68a933310015faa1eb439749f8.png)

在本例中，可得 ![\textstyle \tilde{x}](images/math/1/a/6/1a62e33dcf57261829692126a4dcd02d.png) 的点图如下（取 ![\textstyle n=2, k=1](images/math/9/4/b/94b3c8bb8f57addfc319217446a14d56.png) ）：

![PCA-xtilde.png](images/thumb/2/27/PCA-xtilde.png/600px-PCA-xtilde.png)

然而，由于上面 ![\textstyle \tilde{x}](images/math/1/a/6/1a62e33dcf57261829692126a4dcd02d.png) 的后![\textstyle n-k](images/math/7/4/2/742be0073915ce28ed208c2d5c83fc56.png)项均为零，没必要把这些零项保留下来。所以，我们仅用前 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个（非零）成分来定义 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 维向量 ![\textstyle \tilde{x}](images/math/1/a/6/1a62e33dcf57261829692126a4dcd02d.png) 。

这也解释了我们为什么会以 ![\textstyle u_1, u_2, \ldots, u_n](images/math/d/5/2/d52832ed87962d3ece3043ddae3150a7.png) 为基来表示数据：要决定保留哪些成分变得很简单，只需取前 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个成分即可。这时也可以说，我们“保留了前 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个PCA（主）成分”。

  还原近似数据
--------

现在，我们得到了原始数据 ![\textstyle x \in \Re^n](images/math/9/e/b/9ebd39996afb169318c1dd5fb1503b17.png) 的低维“压缩”表征量 ![\textstyle \tilde{x} \in \Re^k](images/math/2/1/3/21337248295f42f7fe18d9a9b3da57b1.png) ， 反过来，如果给定 ![\textstyle \tilde{x}](images/math/1/a/6/1a62e33dcf57261829692126a4dcd02d.png) ，我们应如何还原原始数据 ![\textstyle x](images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 呢？查看[以往章节](#.E6.97.8B.E8.BD.AC.E6.95.B0.E6.8D.AE)以往章节可知，要转换回来，只需 ![\textstyle x = U x_{\rm rot}](images/math/f/a/a/faada910e82b90d1c221943616cc85ab.png) 即可。进一步，我们把 ![\textstyle \tilde{x}](images/math/1/a/6/1a62e33dcf57261829692126a4dcd02d.png) 看作将 ![\textstyle x_{\rm rot}](images/math/1/7/0/170047e804738636731477291969d554.png) 的最后 ![\textstyle n-k](images/math/7/4/2/742be0073915ce28ed208c2d5c83fc56.png) 个元素被置0所得的近似表示，因此如果给定 ![\textstyle \tilde{x} \in \Re^k](images/math/2/1/3/21337248295f42f7fe18d9a9b3da57b1.png) ，可以通过在其末尾添加 ![\textstyle n-k](images/math/7/4/2/742be0073915ce28ed208c2d5c83fc56.png) 个0来得到对 ![\textstyle x_{\rm rot} \in \Re^n](images/math/f/c/5/fc52a57fe97de0666dc2857bde2df153.png) 的近似，最后，左乘 ![\textstyle U](images/math/6/a/5/6a55fb16b0464ccd6652a7f2a583217f.png) 便可近似还原出原数据 ![\textstyle x](images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 。具体来说，计算如下：

![\begin{align}
\hat{x}  = U \begin{bmatrix} \tilde{x}_1 \\ \vdots \\ \tilde{x}_k \\ 0 \\ \vdots \\ 0 \end{bmatrix}  
= \sum_{i=1}^k u_i \tilde{x}_i.
\end{align}](images/math/0/a/0/0a07b56293a0b63ef434551e9ccda9ea.png)

上面的等式基于[先前](#.E5.AE.9E.E4.BE.8B.E5.92.8C.E6.95.B0.E5.AD.A6.E8.83.8C.E6.99.AF)对 ![\textstyle U](images/math/6/a/5/6a55fb16b0464ccd6652a7f2a583217f.png) 的定义。在实现时，我们实际上并不先给 ![\textstyle \tilde{x}](images/math/1/a/6/1a62e33dcf57261829692126a4dcd02d.png) 填0然后再左乘 ![\textstyle U](images/math/6/a/5/6a55fb16b0464ccd6652a7f2a583217f.png) ，因为这意味着大量的乘0运算。我们可用 ![\textstyle \tilde{x} \in \Re^k](images/math/2/1/3/21337248295f42f7fe18d9a9b3da57b1.png) 来与 ![\textstyle U](images/math/6/a/5/6a55fb16b0464ccd6652a7f2a583217f.png) 的前 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 列相乘，即上式中最右项，来达到同样的目的。将该算法应用于本例中的数据集，可得如下关于重构数据 ![\textstyle \hat{x}](images/math/2/9/0/29035749c12270bcc8de7e36bc459ece.png) 的点图：

![PCA-xhat.png](images/thumb/5/52/PCA-xhat.png/600px-PCA-xhat.png)

由图可见，我们得到的是对原始数据集的一维近似重构。

在训练自动编码器或其它无监督特征学习算法时，算法运行时间将依赖于输入数据的维数。若用 ![\textstyle \tilde{x} \in \Re^k](images/math/2/1/3/21337248295f42f7fe18d9a9b3da57b1.png) 取代 ![\textstyle x](images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 作为输入数据，那么算法就可使用低维数据进行训练，运行速度将显著加快。对于很多数据集来说，低维表征量 ![\textstyle \tilde{x}](images/math/1/a/6/1a62e33dcf57261829692126a4dcd02d.png) 是原数据集的极佳近似，因此在这些场合使用PCA是很合适的，它引入的近似误差的很小，却可显著地提高你算法的运行速度。

  选择主成分个数
---------

我们该如何选择 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) ，即保留多少个PCA主成分？在上面这个简单的二维实验中，保留第一个成分看起来是自然的选择。对于高维数据来说，做这个决定就没那么简单：如果 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 过大，数据压缩率不高，在极限情况 ![\textstyle k=n](images/math/e/3/6/e36b85de9c58866d875f20cbf6fc5f5b.png) 时，等于是在使用原始数据（只是旋转投射到了不同的基）；相反地，如果 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 过小，那数据的近似误差太太。

决定 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 值时，我们通常会考虑不同 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 值可保留的方差百分比。具体来说，如果 ![\textstyle k=n](images/math/e/3/6/e36b85de9c58866d875f20cbf6fc5f5b.png) ，那么我们得到的是对数据的完美近似，也就是保留了100%的方差，即原始数据的所有变化都被保留下来；相反，如果 ![\textstyle k=0](images/math/2/a/2/2a27a4874f5739de5d2947d12ac81d4b.png) ，那等于是使用零向量来逼近输入数据，也就是只有0%的方差被保留下来。

一般而言，设 ![\textstyle \lambda_1, \lambda_2, \ldots, \lambda_n](images/math/d/2/b/d2b02582947d98e3be81be3d1e684f28.png) 表示 ![\textstyle \Sigma](images/math/6/6/9/669ec82a71dede49eb73e539bc3423b6.png) 的特征值（按由大到小顺序排列），使得 ![\textstyle \lambda_j](images/math/c/8/5/c851ef66a35ee95db0b63a592963ca77.png) 为对应于特征向量 ![\textstyle u_j](images/math/d/1/7/d175faaca44b996970abf70b700a94f1.png) 的特征值。那么如果我们保留前 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 个成分，则保留的方差百分比可计算为：

![\begin{align}
\frac{\sum_{j=1}^k \lambda_j}{\sum_{j=1}^n \lambda_j}.
\end{align}](images/math/6/0/b/60ba13aa4527ce9cc11772deaa1d5027.png)

在上面简单的二维实验中，![\textstyle \lambda_1 = 7.29](images/math/6/b/f/6bf8708608604abff35895bb0ecf17f3.png) ，![\textstyle \lambda_2 = 0.69](images/math/5/7/9/5793e844fa46435301414cb62e5d7641.png) 。因此，如果保留 ![\textstyle k=1](images/math/9/7/7/97724a53ab7a652f75e945d2188850d9.png) 个主成分，等于我们保留了 ![\textstyle 7.29/(7.29+0.69) = 0.913](images/math/8/5/9/859dd2ebffe06849e75ce9297f25d325.png) ，即91.3%的方差。

对保留方差的百分比进行更正式的定义已超出了本教程的范围，但很容易证明，![\textstyle \lambda_j =
\sum_{i=1}^m x_{{\rm rot},j}^2](images/math/9/7/e/97ecfffd8596d26deed9542b64cd6712.png) 。因此，如果 ![\textstyle \lambda_j \approx 0](images/math/6/7/1/6716d88c3c1a368824d188c8b9b6b589.png) ，则说明 ![\textstyle x_{{\rm rot},j}](images/math/e/8/4/e84f84acac7b07e18a42a8e91b4433bc.png) 也就基本上接近于0，所以用0来近似它并不会产生多大损失。这也解释了为什么要保留前面的主成分（对应的 ![\textstyle \lambda_j](images/math/c/8/5/c851ef66a35ee95db0b63a592963ca77.png) 值较大）而不是末尾的那些。 这些前面的主成分 ![\textstyle x_{{\rm rot},j}](images/math/e/8/4/e84f84acac7b07e18a42a8e91b4433bc.png) 变化性更大，取值也更大，如果将其设为0势必引入较大的近似误差。

以处理图像数据为例，一个惯常的经验法则是选择 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 以保留99%的方差，换句话说，我们选取满足以下条件的最小 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 值：

![\begin{align}
\frac{\sum_{j=1}^k \lambda_j}{\sum_{j=1}^n \lambda_j} \geq 0.99. 
\end{align}](images/math/7/d/5/7d5f701649af052a671b7d195dccdd8f.png)

对其它应用，如不介意引入稍大的误差，有时也保留90-98%的方差范围。若向他人介绍PCA算法详情，告诉他们你选择的 ![\textstyle k](images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png) 保留了95%的方差，比告诉他们你保留了前120个（或任意某个数字）主成分更好理解。

  对图像数据应用PCA算法
--------------

为使PCA算法能有效工作，通常我们希望所有的特征 ![\textstyle x_1, x_2, \ldots, x_n](images/math/f/2/5/f25d5eb460ed8f894d9be2865a286908.png) 都有相似的取值范围（并且均值接近于0）。如果你曾在其它应用中使用过PCA算法，你可能知道有必要单独对每个特征做预处理，即通过估算每个特征 ![\textstyle x_j](images/math/b/d/f/bdf5b20642553027712d5b5240b31cf3.png) 的均值和方差，而后将其取值范围规整化为零均值和单位方差。但是，对于大部分图像类型，我们却不需要进行这样的预处理。假定我们将在自然图像上训练算法，此时特征 ![\textstyle x_j](images/math/b/d/f/bdf5b20642553027712d5b5240b31cf3.png) 代表的是像素 ![\textstyle j](images/math/2/3/5/235c5146ab110558897640c34dad7d97.png) 的值。所谓“自然图像”，不严格的说，是指人或动物在他们一生中所见的那种图像。

注：通常我们选取含草木等内容的户外场景图片，然后从中随机截取小图像块（如16x16像素）来训练算法。在实践中我们发现，大多数特征学习算法对训练图片的确切类型并不敏感，所以大多数用普通照相机拍摄的图片，只要不是特别的模糊或带有非常奇怪的人工痕迹，都可以使用。

在自然图像上进行训练时，对每一个像素单独估计均值和方差意义不大，因为（理论上）图像任一部分的统计性质都应该和其它部分相同，图像的这种特性被称作平稳性（stationarity）。

具体而言，为使PCA算法正常工作，我们通常需要满足以下要求：(1)特征的均值大致为0；(2)不同特征的方差值彼此相似。对于自然图片，即使不进行方差归一化操作，条件(2)也自然满足，故而我们不再进行任何方差归一化操作（对音频数据,如声谱,或文本数据,如词袋向量，我们通常也不进行方差归一化）。实际上，PCA算法对输入数据具有缩放不变性，无论输入数据的值被如何放大（或缩小），返回的特征向量都不改变。更正式的说：如果将每个特征向量 ![\textstyle x](images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png) 都乘以某个正数（即所有特征量被放大或缩小相同的倍数），PCA的输出特征向量都将不会发生变化。

既然我们不做方差归一化，唯一还需进行的规整化操作就是均值规整化，其目的是保证所有特征的均值都在0附近。根据应用，在大多数情况下，我们并不关注所输入图像的整体明亮程度。比如在对象识别任务中，图像的整体明亮程度并不会影响图像中存在的是什么物体。更为正式地说，我们对图像块的平均亮度值不感兴趣，所以可以减去这个值来进行均值规整化。

具体的步骤是，如果 ![\textstyle x^{(i)} \in \Re^{n}](images/math/c/a/5/ca57b44909d158c3fdfaa849465dd4a2.png) 代表16x16的图像块的亮度（灰度）值（ ![\textstyle n=256](images/math/6/c/0/6c07d223cfb098a75db66924dfcb7210.png) ），可用如下算法来对每幅图像进行零均值化操作：

![\mu^{(i)} := \frac{1}{n} \sum_{j=1}^n x^{(i)}_j](images/math/a/1/0/a104802ef43230cf0d364f378abd2c08.png)

![x^{(i)}_j := x^{(i)}_j - \mu^{(i)}](images/math/6/3/b/63bf04b76d7fffd53d851573573f5f7f.png), for all ![\textstyle j](images/math/2/3/5/235c5146ab110558897640c34dad7d97.png)

请注意：1）对每个输入图像块 ![\textstyle x^{(i)}](images/math/e/b/e/ebe8632b7c91a3dbbf9b590bea887a47.png) 都要单独执行上面两个步骤，2）这里的 ![\textstyle \mu^{(i)}](images/math/c/8/6/c862daa56646826c788aeb8ef0a5e4df.png) 是指图像块 ![\textstyle x^{(i)}](images/math/e/b/e/ebe8632b7c91a3dbbf9b590bea887a47.png) 的平均亮度值。尤其需要注意的是，这和为每个像素 ![\textstyle x_j](images/math/b/d/f/bdf5b20642553027712d5b5240b31cf3.png) 单独估算均值是两个完全不同的概念。

如果你处理的图像并非自然图像（比如，手写文字，或者白背景正中摆放单独物体），其他规整化操作就值得考虑了，而哪种做法最合适也取决于具体应用场合。但对自然图像而言，对每幅图像进行上述的零均值规整化，是默认而合理的处理。

  参考文献
------

[http://cs229.stanford.edu](http://cs229.stanford.edu/)

 中英文对照
------

Principal Components Analysis 主成份分析

whitening 白化

intensity 亮度

mean 平均值

variance 方差

covariance matrix 协方差矩阵

basis 基

magnitude 幅值

stationarity 平稳性

normalization 归一化

eigenvector 特征向量

eigenvalue 特征值

 中文译者
-----

郭亮（guoliang2248@gmail.com），张力（emma.lzhang@gmail.com），金峰（jinfengb@gmail.com）, @破破的桥（新浪微博）, 谭晓阳（x.tan@nuaa.edu.cn）

**主成分分析** | [白化](%E7%99%BD%E5%8C%96.md "白化") | [实现主成分分析和白化](%E5%AE%9E%E7%8E%B0%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E5%92%8C%E7%99%BD%E5%8C%96.md "实现主成分分析和白化") | [Exercise:PCA in 2D](Exercise_PCA_in_2D.md "Exercise:PCA in 2D") | [Exercise:PCA and Whitening](Exercise_PCA_and_Whitening.md "Exercise:PCA and Whitening")

---

> * Language: [English](PCA.md "PCA")
> * This page was last modified on 8 April 2013, at 05:04.

