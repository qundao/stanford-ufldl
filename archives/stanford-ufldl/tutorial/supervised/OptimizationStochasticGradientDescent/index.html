<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Unsupervised Feature Learning and Deep Learning Tutorial</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <link href="/stanford-ufldl/archive/tutorial/css/bootstrap_readable.min.css" rel="stylesheet">
    <link href="/stanford-ufldl/archive/tutorial/css/pygments/default.css" rel="stylesheet">
    <link href="/stanford-ufldl/archive/tutorial/css/custom.css" rel="stylesheet">
    <script type="text/javascript"
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="../../assets/js/html5shiv.js"></script>
    <script src="../../assets/js/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="navbar navbar">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/stanford-ufldl/archive/tutorial/">UFLDL Tutorial</a>
        </div>
        <!---
        <div class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="#">Home</a></li>
          </ul>
        </div>
        !-->
      </div>
    </div>


    <div class="row-fluid">
      <div class = "container"> 
        <div class = "col-xs-9">
          <h3>Optimization: Stochastic Gradient Descent</h3>
<hr>
<!---<p class="meta">28 Aug 2013</p>--->

<div class="post">
<h3 id="overview">Overview</h3>

<p>Batch methods, such as limited memory BFGS, which use the full training set to compute the next update to parameters at each iteration tend to converge very well to local optima. They are also straight forward to get working provided a good off the shelf implementation (e.g. minFunc) because they have very few hyper-parameters to tune. However, often in practice computing the cost and gradient for the entire training set can be very slow and sometimes intractable on a single machine if the dataset is too big to fit in main memory. Another issue with batch optimization methods is that they don’t give an easy way to incorporate new data in an ‘online’ setting. Stochastic Gradient Descent (SGD) addresses both of these issues by following the negative gradient of the objective after seeing only a single or a few training examples. The use of SGD In the neural network setting is motivated by the high cost of running back propagation over the full training set. SGD can overcome this cost and still lead to fast convergence.</p>

<h4 id="stochastic_gradient_descent">Stochastic Gradient Descent</h4>

<p>The standard gradient descent algorithm updates the parameters <m>\theta</m> of the objective <m>J(\theta)</m> as,</p>
<m>
\theta = \theta - \alpha \nabla_\theta E[J(\theta)]
</m>
<p>where the expectation in the above equation is approximated by evaluating the cost and gradient over the full training set. Stochastic Gradient Descent (SGD) simply does away with the expectation in the update and computes the gradient of the parameters using only a single or a few training examples. The new update is given by,</p>
<m>
\theta = \theta - \alpha \nabla_\theta J(\theta; x^{(i)},y^{(i)})
</m>
<p>with a pair <m>(x^{(i)},y^{(i)})</m> from the training set.</p>

<p>Generally each parameter update in SGD is computed w.r.t a few training examples or a minibatch as opposed to a single example. The reason for this is twofold: first this reduces the variance in the parameter update and can lead to more stable convergence, second this allows the computation to take advantage of highly optimized matrix operations that should be used in a well vectorized computation of the cost and gradient. A typical minibatch size is 256, although the optimal size of the minibatch can vary for different applications and architectures.</p>

<p>In SGD the learning rate <m>\alpha</m> is typically much smaller than a corresponding learning rate in batch gradient descent because there is much more variance in the update. Choosing the proper learning rate and schedule (i.e. changing the value of the learning rate as learning progresses) can be fairly difficult. One standard method that works well in practice is to use a small enough constant learning rate that gives stable convergence in the initial epoch (full pass through the training set) or two of training and then halve the value of the learning rate as convergence slows down. An even better approach is to evaluate a held out set after each epoch and anneal the learning rate when the change in objective between epochs is below a small threshold. This tends to give good convergence to a local optima. Another commonly used schedule is to anneal the learning rate at each iteration <m>t</m> as <m>\frac{a}{b+t}</m> where <m>a</m> and <m>b</m> dictate the initial learning rate and when the annealing begins respectively. More sophisticated methods include using a backtracking line search to find the optimal update.</p>

<p>One final but important point regarding SGD is the order in which we present the data to the algorithm. If the data is given in some meaningful order, this can bias the gradient and lead to poor convergence. Generally a good method to avoid this is to randomly shuffle the data prior to each epoch of training.</p>

<h4 id="momentum">Momentum</h4>

<p>If the objective has the form of a long shallow ravine leading to the optimum and steep walls on the sides, standard SGD will tend to oscillate across the narrow ravine since the negative gradient will point down one of the steep sides rather than along the ravine towards the optimum. The objectives of deep architectures have this form near local optima and thus standard SGD can lead to very slow convergence particularly after the initial steep gains. Momentum is one method for pushing the objective more quickly along the shallow ravine. The momentum update is given by,</p>
<m>
\begin{align}
v &amp;= \gamma v+ \alpha \nabla_{\theta} J(\theta; x^{(i)},y^{(i)}) \\
\theta &amp;= \theta - v
\end{align}
</m>
<p>In the above equation <m>v</m> is the current velocity vector which is of the same dimension as the parameter vector <m>\theta</m>. The learning rate <m>\alpha</m> is as described above, although when using momentum <m>\alpha</m> may need to be smaller since the magnitude of the gradient will be larger. Finally <m>\gamma \in (0,1]</m> determines for how many iterations the previous gradients are incorporated into the current update. Generally <m>\gamma</m> is set to 0.5 until the initial learning stabilizes and then is increased to 0.9 or higher.</p>
</div>

        </div>
        <div class = "col-xs-3 sidebar">
          <div id="sidebar-holder">
          </div>
        </div>
      </div>
    </div>

    <!-- Generate sidebar --->
    <script src='/stanford-ufldl/archive/tutorial/sidebar/sidebar.js'></script>
    <script>
      function renderHeading(name) {
        return '<div class = "panel-heading">' + name + '</div>';
      };
      function renderItem(name, link) { 
        return '<a href="/stanford-ufldl/archive/tutorial/' + link + '" class="list-group-item nav">' + name +
          '</a>';
      };
      renderedHtml = '<div class = "panel panel-default">';
        for (var i = 0; i < pages.length; i++) {
          if (pages[i].type == 'Heading') {
            renderedHtml += renderHeading(pages[i].name);
          }
          if (pages[i].type == 'Page') { 
            renderedHtml += renderItem(pages[i].name, pages[i].link);
          }
        };
        renderedHtml += '</div>';
      document.getElementById('sidebar-holder').innerHTML = renderedHtml;
    </script>
    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="//code.jquery.com/jquery.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="/stanford-ufldl/archive/tutorial/js/bootstrap.min.js"></script>
    <script>
      var inputs = document.getElementsByTagName("m");
      for (var i = 0; i < inputs.length; i++) { 
        wrap = '$$';
        if (inputs[i].parentNode.nodeName.toLowerCase() == 'p' || 
        inputs[i].parentNode.nodeName.toLowerCase() == 'li')  {
          wrap = '$'
        }
        inputs[i].textContent = wrap + inputs[i].textContent + wrap;
      }
    </script>
    <script>
      MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          inlineMath: [ ['$','$'], ["\\(","\\)"] ]
        }
      });
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += 'has-jax';
        }
      });
    </script>
    <script>
      /*menu handler*/
      function stripTrailingSlash(str) {
        if(str.substr(-1) == '/') {
          return str.substr(0, str.length - 1);
        }
        return str;
      }

      var url = window.location.href;  
      var activePage = stripTrailingSlash(url);

      $('.nav').each(function(){ 
        currentPage = stripTrailingSlash(this.href) 
        if (activePage == currentPage) {
          $(this).addClass('active'); 
        } 
      });
    </script>
  </body>
</html>