<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Unsupervised Feature Learning and Deep Learning Tutorial</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <link href="/wayback-ufldl/stanford-ufldl/tutorial/css/bootstrap_readable.min.css" rel="stylesheet">
    <link href="/wayback-ufldl/stanford-ufldl/tutorial/css/pygments/default.css" rel="stylesheet">
    <link href="/wayback-ufldl/stanford-ufldl/tutorial/css/custom.css" rel="stylesheet">
    <script type="text/javascript"
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="../../assets/js/html5shiv.js"></script>
    <script src="../../assets/js/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="navbar navbar">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/wayback-ufldl/stanford-ufldl/tutorial/">UFLDL Tutorial</a>
        </div>
        <!---
        <div class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="#">Home</a></li>
          </ul>
        </div>
        !-->
      </div>
    </div>


    <div class="row-fluid">
      <div class = "container"> 
        <div class = "col-xs-9">
          <h3>Linear Regression</h3>
<hr>
<!---<p class="meta">28 Aug 2013</p>--->

<div class="post">
<h3 id="problem_formulation">Problem Formulation</h3>

<p>As a refresher, we will start by learning how to implement linear regression. The main idea is to get familiar with objective functions, computing their gradients and optimizing the objectives over a set of parameters. These basic tools will form the basis for more sophisticated algorithms later. Readers that want additional details may refer to the <a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">Lecture Note</a> on Supervised Learning for more.</p>

<p>Our goal in linear regression is to predict a target value <m>y</m> starting from a vector of input values <m>x \in \Re^n</m>. For example, we might want to make predictions about the price of a house so that <m>y</m> represents the price of the house in dollars and the elements <m>x_j</m> of <m>x</m> represent “features” that describe the house (such as its size and the number of bedrooms). Suppose that we are given many examples of houses where the features for the i’th house are denoted <m>x^{(i)}</m> and the price is <m>y^{(i)}</m>. For short, we will denote the</p>

<p>Our goal is to find a function <m>y = h(x)</m> so that we have <m>y^{(i)} \approx h(x^{(i)})</m> for each training example. If we succeed in finding a function <m>h(x)</m> like this, and we have seen enough examples of houses and their prices, we hope that the function <m>h(x)</m> will also be a good predictor of the house price even when we are given the features for a new house where the price is not known.</p>

<p>To find a function <m>h(x)</m> where <m>y^{(i)} \approx h(x^{(i)})</m> we must first decide how to represent the function <m>h(x)</m>. To start out we will use linear functions: <m>h_\theta(x) = \sum_j \theta_j x_j = \theta^\top x</m>. Here, <m>h_\theta(x)</m> represents a large family of functions parametrized by the choice of <m>\theta</m>. (We call this space of functions a “hypothesis class”.) With this representation for <m>h</m>, our task is to find a choice of <m>\theta</m> so that <m>h_\theta(x^{(i)})</m> is as close as possible to <m>y^{(i)}</m>. In particular, we will search for a choice of <m>\theta</m> that minimizes:</p>
<m>
J(\theta) = \frac{1}{2} \sum_i \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2 = \frac{1}{2} \sum_i \left( \theta^\top x^{(i)} - y^{(i)} \right)^2
</m>
<p>This function is the “cost function” for our problem which measures how much error is incurred in predicting <m>y^{(i)}</m> for a particular choice of <m>\theta</m>. This may also be called a “loss”, “penalty” or “objective” function.</p>

<h3 id="function_minimization">Function Minimization</h3>

<p>We now want to find the choice of <m>\theta</m> that minimizes <m>J(\theta)</m> as given above. There are many algorithms for minimizing functions like this one and we will describe some very effective ones that are easy to implement yourself in a later section <a href="">Gradient descent</a>. For now, let’s take for granted the fact that most commonly-used algorithms for function minimization require us to provide two pieces of information about <m>J(\theta)</m>: We will need to write code to compute <m>J(\theta)</m> and <m>\nabla_\theta J(\theta)</m> on demand for any choice of <m>\theta</m>. After that, the rest of the optimization procedure to find the best choice of <m>\theta</m> will be handled by the optimization algorithm. (Recall that the gradient <m>\nabla_\theta J(\theta)</m> of a differentiable function <m>J</m> is a vector that points in the direction of steepest increase as a function of <m>\theta</m> — so it is easy to see how an optimization algorithm could use this to make a small change to <m>\theta</m> that decreases (or increase) <m>J(\theta)</m>).</p>

<p>The above expression for <m>J(\theta)</m> given a training set of <m>x^{(i)}</m> and <m>y^{(i)}</m> is easy to implement in MATLAB to compute <m>J(\theta)</m> for any choice of <m>\theta</m>. The remaining requirement is to compute the gradient:</p>
<m>\nabla_\theta J(\theta) = \begin{align}\left[\begin{array}{c} \frac{\partial J(\theta)}{\partial \theta_1}  \\
\frac{\partial J(\theta)}{\partial \theta_2}  \\
\vdots\\
\frac{\partial J(\theta)}{\partial \theta_n} \end{array}\right]\end{align}</m>
<p>Differentiating the cost function <m>J(\theta)</m> as given above with respect to a particular parameter <m>\theta_j</m> gives us:</p>
<m>\frac{\partial J(\theta)}{\partial \theta_j} = \sum_i x^{(i)}_j \left(h_\theta(x^{(i)}) - y^{(i)}\right)</m>
<h3 id="exercise_1a_linear_regression">Exercise 1A: Linear Regression</h3>

<p>For this exercise you will implement the objective function and gradient calculations for linear regression in MATLAB.</p>

<p>In the <code>ex1/</code> directory of the starter code package you will find the file <code>ex1_linreg.m</code> which contains the makings of a simple linear regression experiment. This file performs most of the boiler-plate steps for you:</p>

<ol>
<li>
<p>The data is loaded from <code>housing.data</code>. An extra ‘1’ feature is added to the dataset so that <m>\theta_1</m> will act as an intercept term in the linear function.</p>
</li>

<li>
<p>The examples in the dataset are randomly shuffled and the data is then split into a training and testing set. The features that are used as input to the learning algorithm are stored in the variables <code>train.X</code> and <code>test.X</code>. The target value to be predicted is the estimated house price for each example. The prices are stored in “train.y” and “test.y”, respectively, for the training and testing examples. You will use the training set to find the best choice of <m>\theta</m> for predicting the house prices and then check its performance on the testing set.</p>
</li>

<li>
<p>The code calls the minFunc optimization package. minFunc will attempt to find the best choice of <m>\theta</m> by minimizing the objective function implemented in <code>linear_regression.m</code>. It will be your job to implement linear_regression.m to compute the objective function value and the gradient with respect to the parameters.</p>
</li>

<li>
<p>After minFunc completes (i.e., after training is finished), the training and testing error is printed out. Optionally, it will plot a quick visualization of the predicted and actual prices for the examples in the test set.</p>
</li>
</ol>

<p>The <code>ex1_linreg.m</code> file calls the <code>linear_regression.m</code> file that must be filled in with your code. The <code>linear_regression.m</code> file receives the training data <m>X</m>, the training target values (house prices) <m>y</m>, and the current parameters <m>\theta</m>.</p>

<p>Complete the following steps for this exercise:</p>

<ol>
<li>Fill in the <code>linear_regression.m</code> file to compute <m>J(\theta)</m> for the linear regression problem as defined earlier. Store the computed value in the variable <code>f</code>.</li>
</ol>

<p>You may complete both of these steps by looping over the examples in the training set (the columns of the data matrix X) and, for each one, adding its contribution to <code>f</code> and <code>g</code>. We will create a faster version in the next exercise.</p>

<p>Once you complete the exercise successfully, the resulting plot should look something like the one below:</p>

<p><img src='/wayback-ufldl/stanford-ufldl/tutorial/images/House_results.png' /></p>

<p>(Yours may look slightly different depending on the random choice of training and testing sets.) Typical values for the RMS training and testing error are between 4.5 and 5.</p>
</div>

        </div>
        <div class = "col-xs-3 sidebar">
          <div id="sidebar-holder">
          </div>
        </div>
      </div>
    </div>

    <!-- Generate sidebar --->
    <script src='/wayback-ufldl/stanford-ufldl/tutorial/sidebar/sidebar.js'></script>
    <script>
      function renderHeading(name) {
        return '<div class = "panel-heading">' + name + '</div>';
      };
      function renderItem(name, link) { 
        return '<a href="/wayback-ufldl/stanford-ufldl/tutorial/' + link + '" class="list-group-item nav">' + name +
          '</a>';
      };
      renderedHtml = '<div class = "panel panel-default">';
        for (var i = 0; i < pages.length; i++) {
          if (pages[i].type == 'Heading') {
            renderedHtml += renderHeading(pages[i].name);
          }
          if (pages[i].type == 'Page') { 
            renderedHtml += renderItem(pages[i].name, pages[i].link);
          }
        };
        renderedHtml += '</div>';
      document.getElementById('sidebar-holder').innerHTML = renderedHtml;
    </script>
    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="//code.jquery.com/jquery.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="/wayback-ufldl/stanford-ufldl/tutorial/js/bootstrap.min.js"></script>
    <script>
      var inputs = document.getElementsByTagName("m");
      for (var i = 0; i < inputs.length; i++) { 
        wrap = '$$';
        if (inputs[i].parentNode.nodeName.toLowerCase() == 'p' || 
        inputs[i].parentNode.nodeName.toLowerCase() == 'li')  {
          wrap = '$'
        }
        inputs[i].textContent = wrap + inputs[i].textContent + wrap;
      }
    </script>
    <script>
      MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          inlineMath: [ ['$','$'], ["\\(","\\)"] ]
        }
      });
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += 'has-jax';
        }
      });
    </script>
    <script>
      /*menu handler*/
      function stripTrailingSlash(str) {
        if(str.substr(-1) == '/') {
          return str.substr(0, str.length - 1);
        }
        return str;
      }

      var url = window.location.href;  
      var activePage = stripTrailingSlash(url);

      $('.nav').each(function(){ 
        currentPage = stripTrailingSlash(this.href) 
        if (activePage == currentPage) {
          $(this).addClass('active'); 
        } 
      });
    </script>
  </body>
</html>