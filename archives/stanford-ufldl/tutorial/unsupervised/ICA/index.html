<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Unsupervised Feature Learning and Deep Learning Tutorial</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <link href="/wayback-ufldl/stanford-ufldl/tutorial/css/bootstrap_readable.min.css" rel="stylesheet">
    <link href="/wayback-ufldl/stanford-ufldl/tutorial/css/pygments/default.css" rel="stylesheet">
    <link href="/wayback-ufldl/stanford-ufldl/tutorial/css/custom.css" rel="stylesheet">
    <script type="text/javascript"
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="../../assets/js/html5shiv.js"></script>
    <script src="../../assets/js/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="navbar navbar">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/wayback-ufldl/stanford-ufldl/tutorial/">UFLDL Tutorial</a>
        </div>
        <!---
        <div class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="#">Home</a></li>
          </ul>
        </div>
        !-->
      </div>
    </div>


    <div class="row-fluid">
      <div class = "container"> 
        <div class = "col-xs-9">
          <h3>ICA</h3>
<hr>
<!---<p class="meta">28 Aug 2013</p>--->

<div class="post">
<h3 id="introduction">Introduction</h3>

<p>If you recall, in <a href="/wayback-ufldl/stanford-ufldl/tutorial/unsupervised/SparseCoding">Sparse Coding</a>, we wanted to learn an <strong>over-complete</strong> basis for the data. In particular, this implies that the basis vectors that we learn in sparse coding will not be linearly independent. While this may be desirable in certain situations, sometimes we want to learn a linearly independent basis for the data. In independent component analysis (ICA), this is exactly what we want to do. Further, in ICA, we want to learn not just any linearly independent basis, but an <strong>orthonormal</strong> basis for the data. (An orthonormal basis is a basis <m>(\phi_1, \ldots \phi_n)</m> such that <m>\phi_i \cdot \phi_j = 0</m> if <m>i \ne j</m> and <m>1</m> if <m>i = j</m>).</p>

<p>Like sparse coding, independent component analysis has a simple mathematical formulation. Given some data <m>x</m>, we would like to learn a set of basis vectors which we represent in the columns of a matrix <m>W</m>, such that, firstly, as in sparse coding, our features are <strong>sparse</strong>; and secondly, our basis is an <strong>orthonormal</strong> basis. (Note that while in sparse coding, our matrix <m>A</m> was for mapping <strong>features</strong> <m>s</m> to <strong>raw data</strong>, in independent component analysis, our matrix <m>W</m> works in the opposite direction, mapping <strong>raw data</strong> <m>x</m> to <strong>features</strong> instead). This gives us the following objective function:</p>
<m>
J(W) = \lVert Wx \rVert_1 
</m>
<p>This objective function is equivalent to the sparsity penalty on the features <m>s</m> in sparse coding, since <m>Wx</m> is precisely the features that represent the data. Adding in the orthonormality constraint gives us the full optimization problem for independent component analysis:</p>
<m>
\begin{array}{rcl}
     {\rm minimize} &amp; \lVert Wx \rVert_1  \\
     {\rm s.t.}     &amp; WW^T = I \\
\end{array} 
</m>
<p>As is usually the case in deep learning, this problem has no simple analytic solution, and to make matters worse, the orthonormality constraint makes it slightly more difficult to optimize for the objective using gradient descent - every iteration of gradient descent must be followed by a step that maps the new basis back to the space of orthonormal bases (hence enforcing the constraint).</p>

<p>In practice, optimizing for the objective function while enforcing the orthonormality constraint (as described in the section below) is feasible but slow. Hence, the use of orthonormal ICA is limited to situations where it is important to obtain an orthonormal basis.</p>

<h3 id="orthonormal_ica">Orthonormal ICA</h3>

<p>The orthonormal ICA objective is:</p>
<m>
\begin{array}{rcl}
     {\rm minimize} &amp; \lVert Wx \rVert_1  \\
     {\rm s.t.}     &amp; WW^T = I
\end{array} 
</m>
<p>Observe that the constraint <m>WW^T = I</m> implies two other constraints.</p>

<p>Firstly, since we are learning an orthonormal basis, the number of basis vectors we learn must be less than the dimension of the input. In particular, this means that we cannot learn over-complete bases as we usually do in [[Sparse Coding: Autoencoder Interpretation | sparse coding]].</p>

<p>Secondly, the data must be <a href="/wayback-ufldl/stanford-ufldl/tutorial/unsupervised/PCAWhitening">ZCA whitened</a> with no regularization (that is, with <m>\epsilon</m> set to 0).</p>

<p>Hence, before we even begin to optimize for the orthonormal ICA objective, we must ensure that our data has been <strong>whitened</strong>, and that we are learning an <strong>under-complete</strong> basis.</p>

<p>Following that, to optimize for the objective, we can use gradient descent, interspersing gradient descent steps with projection steps to enforce the orthonormality constraint. Hence, the procedure will be as follows:</p>

<p>Repeat until done:</p>
<ol>
<li><m>W \leftarrow W - \alpha \nabla_W \lVert Wx \rVert_1</m></li>
<li><m>W \leftarrow \operatorname{proj}_U W</m> where <m>U</m> is the space of matrices satisfying <m>WW^T = I</m></li>
</ol>
<p>In practice, the learning rate <m>\alpha</m> is varied using a line-search algorithm to speed up the descent, and the projection step is achieved by setting <m>W \leftarrow (WW^T)^{-\frac{1}{2}} W</m>, which can actually be seen as ZCA whitening (<code>TODO</code>: explain how it is like ZCA whitening).</p>
</div>

        </div>
        <div class = "col-xs-3 sidebar">
          <div id="sidebar-holder">
          </div>
        </div>
      </div>
    </div>

    <!-- Generate sidebar --->
    <script src='/wayback-ufldl/stanford-ufldl/tutorial/sidebar/sidebar.js'></script>
    <script>
      function renderHeading(name) {
        return '<div class = "panel-heading">' + name + '</div>';
      };
      function renderItem(name, link) { 
        return '<a href="/wayback-ufldl/stanford-ufldl/tutorial/' + link + '" class="list-group-item nav">' + name +
          '</a>';
      };
      renderedHtml = '<div class = "panel panel-default">';
        for (var i = 0; i < pages.length; i++) {
          if (pages[i].type == 'Heading') {
            renderedHtml += renderHeading(pages[i].name);
          }
          if (pages[i].type == 'Page') { 
            renderedHtml += renderItem(pages[i].name, pages[i].link);
          }
        };
        renderedHtml += '</div>';
      document.getElementById('sidebar-holder').innerHTML = renderedHtml;
    </script>
    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="//code.jquery.com/jquery.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="/wayback-ufldl/stanford-ufldl/tutorial/js/bootstrap.min.js"></script>
    <script>
      var inputs = document.getElementsByTagName("m");
      for (var i = 0; i < inputs.length; i++) { 
        wrap = '$$';
        if (inputs[i].parentNode.nodeName.toLowerCase() == 'p' || 
        inputs[i].parentNode.nodeName.toLowerCase() == 'li')  {
          wrap = '$'
        }
        inputs[i].textContent = wrap + inputs[i].textContent + wrap;
      }
    </script>
    <script>
      MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          inlineMath: [ ['$','$'], ["\\(","\\)"] ]
        }
      });
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += 'has-jax';
        }
      });
    </script>
    <script>
      /*menu handler*/
      function stripTrailingSlash(str) {
        if(str.substr(-1) == '/') {
          return str.substr(0, str.length - 1);
        }
        return str;
      }

      var url = window.location.href;  
      var activePage = stripTrailingSlash(url);

      $('.nav').each(function(){ 
        currentPage = stripTrailingSlash(this.href) 
        if (activePage == currentPage) {
          $(this).addClass('active'); 
        } 
      });
    </script>
  </body>
</html>