<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Unsupervised Feature Learning and Deep Learning Tutorial</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <link href="/wayback-ufldl/stanford-ufldl/tutorial/css/bootstrap_readable.min.css" rel="stylesheet">
    <link href="/wayback-ufldl/stanford-ufldl/tutorial/css/pygments/default.css" rel="stylesheet">
    <link href="/wayback-ufldl/stanford-ufldl/tutorial/css/custom.css" rel="stylesheet">
    <script type="text/javascript"
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="../../assets/js/html5shiv.js"></script>
    <script src="../../assets/js/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="navbar navbar">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/wayback-ufldl/stanford-ufldl/tutorial/">UFLDL Tutorial</a>
        </div>
        <!---
        <div class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="#">Home</a></li>
          </ul>
        </div>
        !-->
      </div>
    </div>


    <div class="row-fluid">
      <div class = "container"> 
        <div class = "col-xs-9">
          <h3>Exercise: PCA Whitening</h3>
<hr>
<!---<p class="meta">28 Aug 2013</p>--->

<div class="post">
<h3 id="pca_and_whitening_on_natural_images">PCA and Whitening on natural images</h3>

<p>In this exercise, you will implement PCA, PCA whitening and ZCA whitening, and apply them to image patches taken from natural images.</p>

<p>You will build on the MATLAB starter code which we have provided in the <a href="https://github.com/amaas/stanford_dl_ex">Github repository</a> You need only write code at the places indicated by <code>YOUR CODE HERE</code> in the files. The only file you need to modify is <code>pca_gen.m</code>.</p>

<h3 id="step_0_prepare_data">Step 0: Prepare data</h3>

<h4 id="step_0a_load_data">Step 0a: Load data</h4>

<p>The starter code contains code to load a set of MNIST images. The raw patches will look something like this:</p>

<p><img src='/wayback-ufldl/stanford-ufldl/tutorial/images/Raw_images.png' width='100%' /></p>

<p>These patches are stored as column vectors <m>x^{(i)} \in \mathbb{R}^{144}</m> in the <m>144 \times 10000</m> matrix <m>x</m>.</p>

<h4 id="step_0b_zero_mean_the_data">Step 0b: Zero mean the data</h4>

<p>First, for each image patch, compute the mean pixel value and subtract it from that image, this centering the image around zero. You should compute a different mean value for each image patch.</p>

<h3 id="step_1_implement_pca">Step 1: Implement PCA</h3>

<h4 id="step_1a_implement_pca">Step 1a: Implement PCA</h4>

<p>In this step, you will implement PCA to obtain <m>x_{\rm rot}</m>, the matrix in which the data is “rotated” to the basis comprising the principal components (i.e. the eigenvectors of <m>\Sigma</m>). Note that in this part of the exercise, you should ”not” whiten the data.</p>

<h4 id="step_1b_check_covariance">Step 1b: Check covariance</h4>

<p>To verify that your implementation of PCA is correct, you should check the covariance matrix for the rotated data <m>x_{\rm rot}</m>. PCA guarantees that the covariance matrix for the rotated data is a diagonal matrix (a matrix with non-zero entries only along the main diagonal). Implement code to compute the covariance matrix and verify this property. One way to do this is to compute the covariance matrix, and visualise it using the MATLAB command <code>imagesc</code>. The image should show a coloured diagonal line against a blue background. For this dataset, because of the range of the diagonal entries, the diagonal line may not be apparent, so you might get a figure like the one show below, but this trick of visualizing using <code>imagesc</code> will come in handy later in this exercise.</p>

<p><img src='/wayback-ufldl/stanford-ufldl/tutorial/images/Pca_covar.png' width='100%' /></p>

<h3 id="step_2_find_number_of_components_to_retain">Step 2: Find number of components to retain</h3>

<p>Next, choose <m>k</m>, the number of principal components to retain. Pick <m>k</m> to be as small as possible, but so that at least 99% of the variance is retained. In the step after this, you will discard all but the top <m>k</m> principal components, reducing the dimension of the original data to <m>k</m>.</p>

<h3 id="step_3_pca_with_dimension_reduction">Step 3: PCA with dimension reduction</h3>

<p>Now that you have found <m>k</m>, compute <m>\tilde{x}</m>, the reduced-dimension representation of the data. This gives you a representation of each image patch as a <m>k</m> dimensional vector instead of a 144 dimensional vector. If you are training a sparse autoencoder or other algorithm on this reduced-dimensional data, it will run faster than if you were training on the original 144 dimensional data.</p>

<p>To see the effect of dimension reduction, go back from <m>\tilde{x}</m> to produce the matrix <m>\hat{x}</m>, the dimension-reduced data but expressed in the original 144 dimensional space of image patches. Visualise <m>\hat{x}</m> and compare it to the raw data, <m>x</m>. You will observe that there is little loss due to throwing away the principal components that correspond to dimensions with low variation. For comparison, you may also wish to generate and visualise <m>\hat{x}</m> for when only 90% of the variance is retained.</p>
<table>
<tr>
<td> <img src='/wayback-ufldl/stanford-ufldl/tutorial/images/Raw_images.png' width='100%' /> </td>
<td> <img src='/wayback-ufldl/stanford-ufldl/tutorial/images/Pca_images.png' width='100%' /> </td> 
<td>
<img src='/wayback-ufldl/stanford-ufldl/tutorial/images/Pca_images_90.png' width='100%' /> 
</td>
</tr>
<tr>
<td>Raw images <br /> &nbsp; </td>
<td>PCA dimension-reduced images<br /> (99% variance)</td>
<td>PCA dimension-reduced images<br /> (90% variance)</td>
</tr>
</table>
<h3 id="step_4_pca_with_whitening_and_regularization">Step 4: PCA with whitening and regularization</h3>

<h4 id="step_4a_implement_pca_with_whitening_and_regularization">Step 4a: Implement PCA with whitening and regularization</h4>

<p>Now implement PCA with whitening and regularization to produce the matrix <m>x_{PCAWhite}</m>. Use the following parameter value:</p>

<p><code>epsilon = 0.1</code></p>

<h4 id="step_4b_check_covariance">Step 4b: Check covariance</h4>

<p>Similar to using PCA alone, PCA with whitening also results in processed data that has a diagonal covariance matrix. However, unlike PCA alone, whitening additionally ensures that the diagonal entries are equal to 1, i.e. that the covariance matrix is the identity matrix.</p>

<p>That would be the case if you were doing whitening alone with no regularization. However, in this case you are whitening with regularization, to avoid numerical/etc. problems associated with small eigenvalues. As a result of this, some of the diagonal entries of the covariance of your <m>x_{\rm PCAwhite}</m> will be smaller than 1.</p>

<p>To verify that your implementation of PCA whitening with and without regularization is correct, you can check these properties. Implement code to compute the covariance matrix and verify this property. (To check the result of PCA without whitening, simply set epsilon to 0, or close to 0, say <code>1e-10</code>). As earlier, you can visualise the covariance matrix with <code>imagesc</code>. When visualised as an image, for PCA whitening without regularization you should see a red line across the diagonal (corresponding to the one entries) against a blue background (corresponding to the zero entries); for PCA whitening with regularization you should see a red line that slowly turns blue across the diagonal (corresponding to the 1 entries slowly becoming smaller).</p>
<table>
 <tr>
 <td>
 <img src='/wayback-ufldl/stanford-ufldl/tutorial/images/Pca_whitened_covar.png' width='100%' />
 </td>
 <td>
 <img src='/wayback-ufldl/stanford-ufldl/tutorial/images/Pca_whitened_unregularised_covar.png' width='100%' />
 </td>
 </tr>
 <tr>
 <td><center>Covariance for PCA whitening with regularization</center></td>
 <td><center>Covariance for PCA whitening without regularization</center></td>
 </tr>
 </table>
<h3 id="step_5_zca_whitening">Step 5: ZCA whitening</h3>

<p>Now implement ZCA whitening to produce the matrix <m>x_{ZCAWhite}</m>. Visualize <m>x_{ZCAWhite}</m> and compare it to the raw data, <m>x</m>. You should observe that whitening results in, among other things, enhanced edges. Try repeating this with <code>epsilon</code> set to 1, 0.1, and 0.01, and see what you obtain. The example shown below (left image) was obtained with <code>epsilon = 0.1</code>.</p>
<table>
 <tr>
 <td>

 <img src='/wayback-ufldl/stanford-ufldl/tutorial/images/Zca_whitened_images.png' width='100%' />
 </td><td>
 <img src='/wayback-ufldl/stanford-ufldl/tutorial/images/Raw_images.png' width='100%' />
 </td>
 </tr>
 <tr>
 <td>ZCA whitened images</td>
 <td>Raw images</td>
 </tr>
 </table>
</div>

        </div>
        <div class = "col-xs-3 sidebar">
          <div id="sidebar-holder">
          </div>
        </div>
      </div>
    </div>

    <!-- Generate sidebar --->
    <script src='/wayback-ufldl/stanford-ufldl/tutorial/sidebar/sidebar.js'></script>
    <script>
      function renderHeading(name) {
        return '<div class = "panel-heading">' + name + '</div>';
      };
      function renderItem(name, link) { 
        return '<a href="/wayback-ufldl/stanford-ufldl/tutorial/' + link + '" class="list-group-item nav">' + name +
          '</a>';
      };
      renderedHtml = '<div class = "panel panel-default">';
        for (var i = 0; i < pages.length; i++) {
          if (pages[i].type == 'Heading') {
            renderedHtml += renderHeading(pages[i].name);
          }
          if (pages[i].type == 'Page') { 
            renderedHtml += renderItem(pages[i].name, pages[i].link);
          }
        };
        renderedHtml += '</div>';
      document.getElementById('sidebar-holder').innerHTML = renderedHtml;
    </script>
    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="//code.jquery.com/jquery.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="/wayback-ufldl/stanford-ufldl/tutorial/js/bootstrap.min.js"></script>
    <script>
      var inputs = document.getElementsByTagName("m");
      for (var i = 0; i < inputs.length; i++) { 
        wrap = '$$';
        if (inputs[i].parentNode.nodeName.toLowerCase() == 'p' || 
        inputs[i].parentNode.nodeName.toLowerCase() == 'li')  {
          wrap = '$'
        }
        inputs[i].textContent = wrap + inputs[i].textContent + wrap;
      }
    </script>
    <script>
      MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          inlineMath: [ ['$','$'], ["\\(","\\)"] ]
        }
      });
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += 'has-jax';
        }
      });
    </script>
    <script>
      /*menu handler*/
      function stripTrailingSlash(str) {
        if(str.substr(-1) == '/') {
          return str.substr(0, str.length - 1);
        }
        return str;
      }

      var url = window.location.href;  
      var activePage = stripTrailingSlash(url);

      $('.nav').each(function(){ 
        currentPage = stripTrailingSlash(this.href) 
        if (activePage == currentPage) {
          $(this).addClass('active'); 
        } 
      });
    </script>
  </body>
</html>