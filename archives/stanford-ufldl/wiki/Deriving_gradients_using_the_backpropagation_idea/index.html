
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deriving gradients using the backpropagation idea - Ufldl</title>
<link rel="stylesheet" href="/wayback-ufldl/stanford-ufldl/wiki/skins/common/shared.css" media="screen"/>
<link rel="stylesheet" href="/wayback-ufldl/stanford-ufldl/wiki/skins/monobook/main.css" media="screen"/>

</head>

<body class="mediawiki ltr ns-0 ns-subject page-Deriving_gradients_using_the_backpropagation_idea skin-monobook">
<div id="globalWrapper">
<div id="column-content"><div id="content">
	<a id="top"></a>
	
	<h1 id="firstHeading" class="firstHeading">Deriving gradients using the backpropagation idea</h1>
	<div id="bodyContent">
		<h3 id="siteSub">From Ufldl</h3>
		<div id="contentSub"></div>
		<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>
<table id="toc" class="toc"><tr><td><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Introduction"><span class="tocnumber">1</span> <span class="toctext">Introduction</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Examples"><span class="tocnumber">2</span> <span class="toctext">Examples</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Example_1:_Objective_for_weight_matrix_in_sparse_coding"><span class="tocnumber">2.1</span> <span class="toctext">Example 1: Objective for weight matrix in sparse coding</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Example_2:_Smoothed_topographic_L1_sparsity_penalty_in_sparse_coding"><span class="tocnumber">2.2</span> <span class="toctext">Example 2: Smoothed topographic L1 sparsity penalty in sparse coding</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#Example_3:_ICA_reconstruction_cost"><span class="tocnumber">2.3</span> <span class="toctext">Example 3: ICA reconstruction cost</span></a></li>
</ul>
</li>
</ul>
</td></tr></table><script>if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } </script>
<h2> <span class="mw-headline" id="Introduction"> Introduction </span></h2>
<p>In the section on the <a href="/wayback-ufldl/stanford-ufldl/wiki/Backpropagation_Algorithm" title="Backpropagation Algorithm"> backpropagation algorithm</a>, you were briefly introduced to backpropagation as a means of deriving gradients for learning in the sparse autoencoder. It turns out that together with matrix calculus, this provides a powerful method and intuition for deriving gradients for more complex matrix functions (functions from matrices to the reals, or symbolically, from <img class="tex" alt="\mathbb{R}^{r \times c} \rightarrow \mathbb{R}" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/5/b/3/5b3a7630692b07263c08fac96c88c98e.png"/>).
</p><p>First, recall the backpropagation idea, which we present in a modified form appropriate for our purposes below:
</p>
<ol>
<li>For each output unit <span class="texhtml"><i>i</i></span> in layer <span class="texhtml"><i>n</i><sub><i>l</i></sub></span> (the final layer), set
<dl><dd><img class="tex" alt="
\delta^{(n_l)}_i
= \frac{\partial}{\partial z^{(n_l)}_i} \;\;
        J(z^{(n_l)})
" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/1/3/c/13cbf81577c102ed2e01d67f71723076.png"/>
</dd></dl>
where <span class="texhtml"><i>J</i>(<i>z</i>)</span> is our "objective function" (explained below).
<li>For <img class="tex" alt="l = n_l-1, n_l-2, n_l-3, \ldots, 2" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/9/8/8/988861db3f04c9f1150b482aca116daa.png"/> 
<dl><dd>For each node <span class="texhtml"><i>i</i></span> in layer <span class="texhtml"><i>l</i></span>, set
<dl><dd><img class="tex" alt="
                 \delta^{(l)}_i = \left( \sum_{j=1}^{s_{l+1}} W^{(l)}_{ji} \delta^{(l+1)}_j \right) \bullet \frac{\partial}{\partial z^{(l)}_i} f^{(l)} (z^{(l)}_i)
" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/9/4/7/947031b9c9f1be0fc792bf2a1b98c27d.png"/>
</dd></dl>
</dd></dl>
<li>Compute the desired partial derivatives,
<dl><dd><img class="tex" alt="
\begin{align}
\nabla_{W^{(l)}} J(W,b;x,y) &amp;= \delta^{(l+1)} (a^{(l)})^T, \\
\end{align}
" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/5/a/3/5a34eec4ca6a8dd244ed4497cd78ad63.png"/>
</dd></dl>
</ol>
<p>Quick notation recap: 
</p>
<ul>
<li><span class="texhtml"><i>l</i></span> is the number of layers in the neural network
<li><span class="texhtml"><i>n</i><sub><i>l</i></sub></span> is the number of neurons in the <span class="texhtml"><i>l</i></span>th layer
<li><img class="tex" alt="W^{(l)}_{ji}" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/3/6/1/36184dd6c51daad9e5c9f1973933460e.png"/> is the weight from the <span class="texhtml"><i>i</i></span>th unit in the <span class="texhtml"><i>l</i></span>th layer to the <span class="texhtml"><i>j</i></span>th unit in the <span class="texhtml">(<i>l</i> + 1)</span>th layer
<li><img class="tex" alt="z^{(l)}_i" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/0/5/3/053932a35e5e7923d66bfd5cbc15b280.png"/> is the input to the <span class="texhtml"><i>i</i></span>th unit in the <span class="texhtml"><i>l</i></span>th layer
<li><img class="tex" alt="a^{(l)}_i" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/2/f/1/2f12132475b24d761ca573173962be9b.png"/> is the activation of the <span class="texhtml"><i>i</i></span>th unit in the <span class="texhtml"><i>l</i></span>th layer
<li><img class="tex" alt="A \bullet B" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/0/3/c/03caf6030df47b28250decb7a399c191.png"/> is the Hadamard or element-wise product, which for <img class="tex" alt="r \times c" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/f/5/b/f5b34ce727a51879b69d50dbb38cec68.png"/> matrices <span class="texhtml"><i>A</i></span> and <span class="texhtml"><i>B</i></span> yields the <img class="tex" alt="r \times c" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/f/5/b/f5b34ce727a51879b69d50dbb38cec68.png"/> matrix <img class="tex" alt="C = A \bullet B" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/d/b/f/dbf40e2ec518a8d773f3b648f9bd4b7d.png"/> such that <img class="tex" alt="C_{r, c} = A_{r, c} \cdot B_{r, c}" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/9/b/2/9b25139003c1d65c569180099b9e56a7.png"/>
<li><span class="texhtml"><i>f</i><sup>(<i>l</i>)</sup></span> is the activation function for units in the <span class="texhtml"><i>l</i></span>th layer
</ul>
<p>Let's say we have a function <span class="texhtml"><i>F</i></span> that takes a matrix <span class="texhtml"><i>X</i></span> and yields a real number. We would like to use the backpropagation idea to compute the gradient with respect to <span class="texhtml"><i>X</i></span> of <span class="texhtml"><i>F</i></span>, that is <img class="tex" alt="\nabla_X F" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/c/8/a/c8a57f802f72156c4dbee1bd9fde338e.png"/>. The general idea is to see the function <span class="texhtml"><i>F</i></span> as a multi-layer neural network, and to derive the gradients using the backpropagation idea. 
</p><p>To do this, we will set our "objective function" to be the function <span class="texhtml"><i>J</i>(<i>z</i>)</span> that when applied to the outputs of the neurons in the last layer yields the value <span class="texhtml"><i>F</i>(<i>X</i>)</span>. For the intermediate layers, we will also choose our activation functions <span class="texhtml"><i>f</i><sup>(<i>l</i>)</sup></span> to this end.
</p><p>Using this method, we can easily compute derivatives with respect to the inputs <span class="texhtml"><i>X</i></span>, as well as derivatives with respect to any of the weights in the network, as we shall see later.
</p>
<h2> <span class="mw-headline" id="Examples"> Examples </span></h2>
<p>To illustrate the use of the backpropagation idea to compute derivatives with respect to the inputs, we will use two functions from the section on <a href="/wayback-ufldl/stanford-ufldl/wiki/Sparse_Coding__Autoencoder_Interpretation" title="Sparse Coding: Autoencoder Interpretation"> sparse coding</a>, in examples 1 and 2. In example 3, we use a function from <a href="/wayback-ufldl/stanford-ufldl/wiki/Independent_Component_Analysis" title="Independent Component Analysis"> independent component analysis</a> to illustrate the use of this idea to compute derivates with respect to weights, and in this specific case, what to do in the case of tied or repeated weights.
</p>
<h3> <span class="mw-headline" id="Example_1:_Objective_for_weight_matrix_in_sparse_coding"> Example 1: Objective for weight matrix in sparse coding </span></h3>
<p>Recall for <a href="/wayback-ufldl/stanford-ufldl/wiki/Sparse_Coding__Autoencoder_Interpretation" title="Sparse Coding: Autoencoder Interpretation"> sparse coding</a>, the objective function for the weight matrix <span class="texhtml"><i>A</i></span>, given the feature matrix <span class="texhtml"><i>s</i></span>:
</p>
<dl><dd><img class="tex" alt="F(A; s) = \lVert As - x \rVert_2^2 + \gamma \lVert A \rVert_2^2" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/d/8/a/d8a544d689b8b25c191b77b5010f2e98.png"/>
</dd></dl>
<p>We would like to find the gradient of <span class="texhtml"><i>F</i></span> with respect to <span class="texhtml"><i>A</i></span>, or in symbols, <img class="tex" alt="\nabla_A F(A)" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/b/c/2/bc2d77b08b71888b46b4cc02b319a8d5.png"/>. Since the objective function is a sum of two terms in <span class="texhtml"><i>A</i></span>, the gradient is the sum of gradients of each of the individual terms. The gradient of the second term is trivial, so we will consider the gradient of the first term instead. 
</p><p>The first term, <img class="tex" alt="\lVert As - x \rVert_2^2" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/7/d/2/7d2f077de4b218982f04826f6f5a91aa.png"/>, can be seen as an instantiation of neural network taking <span class="texhtml"><i>s</i></span> as an input, and proceeding in four steps, as described and illustrated in the paragraph and diagram below:
</p>
<ol>
<li>Apply <span class="texhtml"><i>A</i></span> as the weights from the first layer to the second layer.
<li>Subtract <span class="texhtml"><i>x</i></span> from the activation of the second layer, which uses the identity activation function.
<li>Pass this unchanged to the third layer, via identity weights. Use the square function as the activation function for the third layer.
<li>Sum all the activations of the third layer.
</ol>
<p><a href="" class="image"><img alt="Backpropagation Method Example 1.png" src="/wayback-ufldl/stanford-ufldl/wiki/images/thumb/b/bd/Backpropagation_Method_Example_1.png/400px-Backpropagation_Method_Example_1.png" width="400" height="380"/></a>
</p><p>The weights and activation functions of this network are as follows:
</p>
<table align="center">
<tr><th width="50px">Layer</th><th width="200px">Weight</th><th width="200px">Activation function <span class="texhtml"><i>f</i></span></th></tr>
<tr>
<td>1</td>
<td><span class="texhtml"><i>A</i></span></td>
<td><span class="texhtml"><i>f</i>(<i>z</i><sub><i>i</i></sub>) = <i>z</i><sub><i>i</i></sub></span> (identity)</td>
</tr>
<tr>
<td>2</td>
<td><span class="texhtml"><i>I</i></span> (identity)</td>
<td><span class="texhtml"><i>f</i>(<i>z</i><sub><i>i</i></sub>) = <i>z</i><sub><i>i</i></sub> &minus; <i>x</i><sub><i>i</i></sub></span></td>
</tr>
<tr>
<td>3</td>
<td>N/A</td>
<td><img class="tex" alt="f(z_i) = z_i^2" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/4/d/b/4dbeed1b426fc9b28e3903789a481ede.png"/></td>
</tr>
</table>
<p>To have <span class="texhtml"><i>J</i>(<i>z</i><sup>(3)</sup>) = <i>F</i>(<i>x</i>)</span>, we can set <img class="tex" alt="J(z^{(3)}) = \sum_k J(z^{(3)}_k)" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/c/4/e/c4e22c48b65f68377d01e81d6312b145.png"/>.
</p><p>Once we see <span class="texhtml"><i>F</i></span> as a neural network, the gradient <img class="tex" alt="\nabla_X F" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/c/8/a/c8a57f802f72156c4dbee1bd9fde338e.png"/> becomes easy to compute - applying backpropagation yields:
</p>
<table align="center">
<tr><th width="50px">Layer</th><th width="200px">Derivative of activation function <span class="texhtml"><i>f</i>'</span></th><th width="200px">Delta</th><th>Input <span class="texhtml"><i>z</i></span> to this layer</th></tr>
<tr>
<td>3</td>
<td><span class="texhtml"><i>f</i>'(<i>z</i><sub><i>i</i></sub>) = 2<i>z</i><sub><i>i</i></sub></span></td>
<td><span class="texhtml"><i>f</i>'(<i>z</i><sub><i>i</i></sub>) = 2<i>z</i><sub><i>i</i></sub></span></td>
<td><span class="texhtml"><i>A</i><i>s</i> &minus; <i>x</i></span></td>
</tr>
<tr>
<td>2</td>
<td><span class="texhtml"><i>f</i>'(<i>z</i><sub><i>i</i></sub>) = 1</span></td>
<td><img class="tex" alt="\left( I^T \delta^{(3)} \right) \bullet 1" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/6/f/7/6f744c80bb2283af18f61dace3f51daf.png"/></td>
<td><span class="texhtml"><i>A</i><i>s</i></span></td>
</tr>
<tr>
<td>1</td>
<td><span class="texhtml"><i>f</i>'(<i>z</i><sub><i>i</i></sub>) = 1</span></td>
<td><img class="tex" alt="\left( A^T \delta^{(2)} \right) \bullet 1" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/a/2/a/a2aace0cbcafb22f600ca0c286cd34ac.png"/></td>
<td><span class="texhtml"><i>s</i></span></td>
</tr>
</table>
<p>Hence, 
</p>
<dl><dd><img class="tex" alt="
\begin{align}
\nabla_X F &amp; = A^T I^T 2(As - x) \\
&amp; = A^T 2(As - x)
\end{align}
" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/3/5/a/35a198eeea379f6e5fddd29fe4a6c2d7.png"/> 
</dd></dl>
<h3> <span class="mw-headline" id="Example_2:_Smoothed_topographic_L1_sparsity_penalty_in_sparse_coding"> Example 2: Smoothed topographic L1 sparsity penalty in sparse coding  </span></h3>
<p>Recall the smoothed topographic L1 sparsity penalty on <span class="texhtml"><i>s</i></span> in <a href="/wayback-ufldl/stanford-ufldl/wiki/Sparse_Coding__Autoencoder_Interpretation" title="Sparse Coding: Autoencoder Interpretation"> sparse coding</a>:
</p>
<dl><dd><img class="tex" alt="\sum{ \sqrt{Vss^T + \epsilon} }" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/c/c/d/ccd5a0f991db6bdba852b147ee42d91b.png"/>
</dd></dl>
<p>where <span class="texhtml"><i>V</i></span> is the grouping matrix, <span class="texhtml"><i>s</i></span> is the feature matrix and <span class="texhtml">&epsilon;</span> is a constant.
</p><p>We would like to find <img class="tex" alt="\nabla_s \sum{ \sqrt{Vss^T + \epsilon} }" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/2/3/c/23c8b28a984cc20529d2eff361fbbe91.png"/>. As above, let's see this term as an instantiation of a neural network:
</p><p><a href="" class="image"><img alt="Backpropagation Method Example 2.png" src="/wayback-ufldl/stanford-ufldl/wiki/images/thumb/5/57/Backpropagation_Method_Example_2.png/600px-Backpropagation_Method_Example_2.png" width="600" height="414"/></a>
</p><p>The weights and activation functions of this network are as follows:
</p>
<table align="center">
<tr><th width="50px">Layer</th><th width="200px">Weight</th><th width="200px">Activation function <span class="texhtml"><i>f</i></span></th></tr>
<tr>
<td>1</td>
<td><span class="texhtml"><i>I</i></span></td>
<td><img class="tex" alt="f(z_i) = z_i^2" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/4/d/b/4dbeed1b426fc9b28e3903789a481ede.png"/></td>
</tr>
<tr>
<td>2</td>
<td><span class="texhtml"><i>V</i></span></td>
<td><span class="texhtml"><i>f</i>(<i>z</i><sub><i>i</i></sub>) = <i>z</i><sub><i>i</i></sub></span></td>
</tr>
<tr>
<td>3</td>
<td><span class="texhtml"><i>I</i></span></td>
<td><span class="texhtml"><i>f</i>(<i>z</i><sub><i>i</i></sub>) = <i>z</i><sub><i>i</i></sub> + &epsilon;</span></td>
</tr>
<tr>
<td>4</td>
<td>N/A</td>
<td><img class="tex" alt="f(z_i) = z_i^{\frac{1}{2}}" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/f/2/f/f2f3700ca152ab5d67ee52aacb2a386d.png"/></td>
</tr>
</table>
<p>To have <span class="texhtml"><i>J</i>(<i>z</i><sup>(4)</sup>) = <i>F</i>(<i>x</i>)</span>, we can set <img class="tex" alt="J(z^{(4)}) = \sum_k J(z^{(4)}_k)" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/5/c/c/5cc78742561e48008ea2fdc832873d87.png"/>.
</p><p>Once we see <span class="texhtml"><i>F</i></span> as a neural network, the gradient <img class="tex" alt="\nabla_X F" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/c/8/a/c8a57f802f72156c4dbee1bd9fde338e.png"/> becomes easy to compute - applying backpropagation yields:
</p>
<table align="center">
<tr><th width="50px">Layer</th><th width="200px">Derivative of activation function <span class="texhtml"><i>f</i>'</span>
</th><th width="200px">Delta</th><th>Input <span class="texhtml"><i>z</i></span> to this layer</th></tr>
<tr>
<td>4</td>
<td><img class="tex" alt="f'(z_i) = \frac{1}{2} z_i^{-\frac{1}{2}}" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/7/c/c/7cc2c62a26d215a7fa8f8207ed608ac2.png"/></td>
<td><img class="tex" alt="f'(z_i) = \frac{1}{2} z_i^{-\frac{1}{2}}" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/7/c/c/7cc2c62a26d215a7fa8f8207ed608ac2.png"/></td>
<td><span class="texhtml">(<i>V</i><i>s</i><i>s</i><sup><i>T</i></sup> + &epsilon;)</span></td>
</tr>
<tr>
<td>3</td>
<td><span class="texhtml"><i>f</i>'(<i>z</i><sub><i>i</i></sub>) = 1</span></td>
<td><img class="tex" alt="\left( I^T \delta^{(4)} \right) \bullet 1" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/1/0/0/1003bad489ec177dfb4d21f0fb28aa33.png"/></td>
<td><span class="texhtml"><i>V</i><i>s</i><i>s</i><sup><i>T</i></sup></span></td>
</tr>
<tr>
<td>2</td>
<td><span class="texhtml"><i>f</i>'(<i>z</i><sub><i>i</i></sub>) = 1</span></td>
<td><img class="tex" alt="\left( V^T \delta^{(3)} \right) \bullet 1" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/4/4/8/448bee03baea6c9d1c1424ca51950110.png"/></td>
<td><span class="texhtml"><i>s</i><i>s</i><sup><i>T</i></sup></span></td>
</tr>
<tr>
<td>1</td>
<td><span class="texhtml"><i>f</i>'(<i>z</i><sub><i>i</i></sub>) = 2<i>z</i><sub><i>i</i></sub></span></td>
<td><img class="tex" alt="\left( I^T \delta^{(2)} \right) \bullet 2s" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/7/c/3/7c37119eae8bb947942dbd49fa994625.png"/></td>
<td><span class="texhtml"><i>s</i></span></td>
</tr>
</table>
<p>Hence, 
</p>
<dl><dd><img class="tex" alt="
\begin{align}
\nabla_X F &amp; = I^T V^T I^T \frac{1}{2}(Vss^T + \epsilon)^{-\frac{1}{2}} \bullet 2s \\
&amp; = V^T \frac{1}{2}(Vss^T + \epsilon)^{-\frac{1}{2}} \bullet 2s \\
&amp; = V^T (Vss^T + \epsilon)^{-\frac{1}{2}} \bullet s
\end{align}
" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/c/0/1/c01e5b899a859c62c2a9de3d9e1bff34.png"/>
</dd></dl>
<h3> <span class="mw-headline" id="Example_3:_ICA_reconstruction_cost"> Example 3: ICA reconstruction cost </span></h3>
<p>Recall the <a href="/wayback-ufldl/stanford-ufldl/wiki/Independent_Component_Analysis" title="Independent Component Analysis"> independent component analysis (ICA)</a> reconstruction cost term:
<img class="tex" alt="\lVert W^TWx - x \rVert_2^2" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/c/9/8/c981b116dd26204d280f18b707c38a2c.png"/>
where <span class="texhtml"><i>W</i></span> is the weight matrix and <span class="texhtml"><i>x</i></span> is the input.
</p><p>We would like to find <img class="tex" alt="\nabla_W \lVert W^TWx - x \rVert_2^2" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/c/1/0/c10b279f6aea106e455f113f8f3ab2c7.png"/> - the derivative of the term with respect to the <b>weight matrix</b>, rather than the <b>input</b> as in the earlier two examples. We will still proceed similarly though, seeing this term as an instantiation of a neural network:
</p><p><a href="" class="image"><img alt="Backpropagation Method Example 3.png" src="/wayback-ufldl/stanford-ufldl/wiki/images/thumb/9/9e/Backpropagation_Method_Example_3.png/400px-Backpropagation_Method_Example_3.png" width="400" height="217"/></a>
</p><p>The weights and activation functions of this network are as follows:
</p>
<table align="center">
<tr><th width="50px">Layer</th><th width="200px">Weight</th><th width="200px">Activation function <span class="texhtml"><i>f</i></span></th></tr>
<tr>
<td>1</td>
<td><span class="texhtml"><i>W</i></span></td>
<td><span class="texhtml"><i>f</i>(<i>z</i><sub><i>i</i></sub>) = <i>z</i><sub><i>i</i></sub></span></td>
</tr>
<tr>
<td>2</td>
<td><span class="texhtml"><i>W</i><sup><i>T</i></sup></span></td>
<td><span class="texhtml"><i>f</i>(<i>z</i><sub><i>i</i></sub>) = <i>z</i><sub><i>i</i></sub></span></td>
</tr>
<tr>
<td>3</td>
<td><span class="texhtml"><i>I</i></span></td>
<td><span class="texhtml"><i>f</i>(<i>z</i><sub><i>i</i></sub>) = <i>z</i><sub><i>i</i></sub> &minus; <i>x</i><sub><i>i</i></sub></span></td>
</tr>
<tr>
<td>4</td>
<td>N/A</td>
<td><img class="tex" alt="f(z_i) = z_i^2" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/4/d/b/4dbeed1b426fc9b28e3903789a481ede.png"/></td>
</tr>
</table>
<p>To have <span class="texhtml"><i>J</i>(<i>z</i><sup>(4)</sup>) = <i>F</i>(<i>x</i>)</span>, we can set <img class="tex" alt="J(z^{(4)}) = \sum_k J(z^{(4)}_k)" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/5/c/c/5cc78742561e48008ea2fdc832873d87.png"/>.
</p><p>Now that we can see <span class="texhtml"><i>F</i></span> as a neural network, we can try to compute the gradient <img class="tex" alt="\nabla_W F" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/e/7/3/e7379e93c2fe4b318c07026bd7adb4ab.png"/>. However, we now face the difficulty that <span class="texhtml"><i>W</i></span> appears twice in the network. Fortunately, it turns out that if <span class="texhtml"><i>W</i></span> appears multiple times in the network, the gradient with respect to <span class="texhtml"><i>W</i></span> is simply the sum of gradients for each instance of <span class="texhtml"><i>W</i></span> in the network (you may wish to work out a formal proof of this fact to convince yourself). With this in mind, we will proceed to work out the deltas first:
</p>
<table align="center">
<tr><th width="50px">Layer</th><th width="200px">Derivative of activation function <span class="texhtml"><i>f</i>'</span>
</th><th width="200px">Delta</th><th>Input <span class="texhtml"><i>z</i></span> to this layer</th></tr>
<tr>
<td>4</td>
<td><span class="texhtml"><i>f</i>'(<i>z</i><sub><i>i</i></sub>) = 2<i>z</i><sub><i>i</i></sub></span></td>
<td><span class="texhtml"><i>f</i>'(<i>z</i><sub><i>i</i></sub>) = 2<i>z</i><sub><i>i</i></sub></span></td>
<td><span class="texhtml">(<i>W</i><sup><i>T</i></sup><i>W</i><i>x</i> &minus; <i>x</i>)</span></td>
</tr>
<tr>
<td>3</td>
<td><span class="texhtml"><i>f</i>'(<i>z</i><sub><i>i</i></sub>) = 1</span></td>
<td><img class="tex" alt="\left( I^T \delta^{(4)} \right) \bullet 1" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/1/0/0/1003bad489ec177dfb4d21f0fb28aa33.png"/></td>
<td><span class="texhtml"><i>W</i><sup><i>T</i></sup><i>W</i><i>x</i></span></td>
</tr>
<tr>
<td>2</td>
<td><span class="texhtml"><i>f</i>'(<i>z</i><sub><i>i</i></sub>) = 1</span></td>
<td><img class="tex" alt="\left( (W^T)^T \delta^{(3)} \right) \bullet 1" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/a/5/5/a55a8b7c6e47321b78ed05c829066cb7.png"/></td>
<td><span class="texhtml"><i>W</i><i>x</i></span></td>
</tr>
<tr>
<td>1</td>
<td><span class="texhtml"><i>f</i>'(<i>z</i><sub><i>i</i></sub>) = 1</span></td>
<td><img class="tex" alt="\left( W^T \delta^{(2)} \right) \bullet 1" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/b/3/0/b30a81c1e4eaf5c73a17572300a1310e.png"/></td>
<td><span class="texhtml"><i>x</i></span></td>
</tr>
</table>
<p>To find the gradients with respect to <span class="texhtml"><i>W</i></span>, first we find the gradients with respect to each instance of <span class="texhtml"><i>W</i></span> in the network.
</p><p>With respect to <span class="texhtml"><i>W</i><sup><i>T</i></sup></span>:
</p>
<dl><dd><img class="tex" alt="
\begin{align}
\nabla_{W^T} F &amp; = \delta^{(3)} a^{(2)T} \\
&amp; = 2(W^TWx - x) (Wx)^T
\end{align}
" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/9/9/3/993726ae12c879a47221ef98d5278c7d.png"/>
</dd></dl>
<p>With respect to <span class="texhtml"><i>W</i></span>:
</p>
<dl><dd><img class="tex" alt="
\begin{align}
\nabla_{W} F &amp; = \delta^{(2)} a^{(1)T} \\
&amp; = (W^T)(2(W^TWx -x)) x^T
\end{align}
" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/0/4/9/049ff65a0c5539792da624939122acb9.png"/>
</dd></dl>
<p>Taking sums, noting that we need to transpose the gradient with respect to <span class="texhtml"><i>W</i><sup><i>T</i></sup></span> to get the gradient with respect to <span class="texhtml"><i>W</i></span>, yields the final gradient with respect to <span class="texhtml"><i>W</i></span> (pardon the slight abuse of notation here):
</p>
<dl><dd><img class="tex" alt="
\begin{align}
\nabla_{W} F &amp; = \nabla_{W} F + (\nabla_{W^T} F)^T \\
&amp; = (W^T)(2(W^TWx -x)) x^T + 2(Wx)(W^TWx - x)^T
\end{align}
" src="/wayback-ufldl/stanford-ufldl/wiki/images/math/c/0/f/c0f24f7a4b6928641a9bc10318b6b85d.png"/>
</dd></dl>
<p><br/>
</p>
<div style="text-align: left;font-size:small;background-color: #eeeeee; border-style: solid; border-width: 1px; padding: 5px">
<p>Language&nbsp;: <a href="/wayback-ufldl/stanford-ufldl/wiki/%E7%94%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E6%80%9D%E6%83%B3%E6%B1%82%E5%AF%BC" title="用反向传导思想求导">中文</a>
</p>
</div>

<!-- 
NewPP limit report
Preprocessor node count: 1162/1000000
Post-expand include size: 183/2097152 bytes
Template argument size: 33/2097152 bytes
Expensive parser function count: 0/100
-->

<div class="printfooter">
</div>		<div id="catlinks" class="catlinks catlinks-allhidden"></div>		<!-- end content -->
				<div class="visualClear"></div>
	</div>
</div></div>
<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
				 <li id="ca-nstab-main" class="selected"><a href="/wayback-ufldl/stanford-ufldl/wiki/Deriving_gradients_using_the_backpropagation_idea" title="View the content page [c]" accesskey="c">Page</a></li>
				 <li id="ca-talk" class="new"><a href="" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-viewsource"><a href="" title="This page is protected.
You can view its source [e]" accesskey="e">View source</a></li>
				 <li id="ca-history"><a href="" title="Past revisions of this page [h]" accesskey="h">History</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(/wayback-ufldl/stanford-ufldl/wiki/skins/common/images/dolphin-openclipart.png);" href="/wayback-ufldl/stanford-ufldl/wiki/Main_Page" title="Visit the main page"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class="generated-sidebar portlet" id="p-ufldl_resources">
		<h5>ufldl resources</h5>
		<div class="pBody">
			<ul>
				<li id="n-UFLDL-Tutorial"><a href="/wayback-ufldl/stanford-ufldl/wiki/UFLDL_Tutorial">UFLDL Tutorial</a></li>
				<li id="n-Recommended-Readings"><a href="/wayback-ufldl/stanford-ufldl/wiki/UFLDL_Recommended_Readings">Recommended Readings</a></li>
			</ul>
		</div>
	</div>
	<div class="generated-sidebar portlet" id="p-wiki">
		<h5>wiki</h5>
		<div class="pBody">
			<ul>
				<li id="n-mainpage-description"><a href="/wayback-ufldl/stanford-ufldl/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-recentchanges"><a href="" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-randompage"><a href="" title="Load a random page [x]" accesskey="x">Random page</a></li>
				<li id="n-help"><a href="" title="The place to find out">Help</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="" id="searchform">
				<input type="hidden" name="title" value="Special:Search" disabled/>
				<input id="searchInput" title="Search Ufldl" accesskey="f" type="search" name="search" disabled/>
				<input type="submit" name="go" class="searchButton" id="searchGoButton" value="Go" title="Go to a page with this exact name if exists" disabled/>&nbsp;
				<input type="submit" name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search the pages for this text" disabled/>
			</form>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="" title="List of all wiki pages that link here [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-specialpages"><a href="" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="" title="Permanent link to this revision of the page">Permanent link</a></li>			</ul>
		</div>
	</div>
</div><!-- end of the left (by default at least) column -->
<div class="visualClear"></div>
<div id="footer">
	<ul id="f-list">
		<li id="lastmod"> This page was last modified on 8 April 2013, at 04:26.</li>
		<li id="privacy"><a href="/wayback-ufldl/stanford-ufldl/wiki/Ufldl_Privacy_policy" title="Ufldl:Privacy policy">Privacy policy</a></li>
		<li id="about"><a href="/wayback-ufldl/stanford-ufldl/wiki/Ufldl_About" title="Ufldl:About">About Ufldl</a></li>
		<li id="disclaimer"><a href="/wayback-ufldl/stanford-ufldl/wiki/Ufldl_General_disclaimer" title="Ufldl:General disclaimer">Disclaimers</a></li>
	</ul>
</div>
</div>

<!-- Served in 0.128 secs. -->
</body>
</html>
