
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Whitening - Ufldl</title>
<link rel="stylesheet" href="/stanford-ufldl/archive/wiki/skins/common/shared.css" media="screen"/>
<link rel="stylesheet" href="/stanford-ufldl/archive/wiki/skins/monobook/main.css" media="screen"/>

</head>

<body class="mediawiki ltr ns-0 ns-subject page-Whitening skin-monobook">
<div id="globalWrapper">
<div id="column-content"><div id="content">
	<a id="top"></a>
	
	<h1 id="firstHeading" class="firstHeading">Whitening</h1>
	<div id="bodyContent">
		<h3 id="siteSub">From Ufldl</h3>
		<div id="contentSub"></div>
		<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>
<table id="toc" class="toc"><tr><td><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Introduction"><span class="tocnumber">1</span> <span class="toctext">Introduction</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#2D_example"><span class="tocnumber">2</span> <span class="toctext">2D example</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#ZCA_Whitening"><span class="tocnumber">3</span> <span class="toctext">ZCA Whitening</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#Regularizaton"><span class="tocnumber">4</span> <span class="toctext">Regularizaton</span></a></li>
</ul>
</td></tr></table><script>if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } </script>
<h2> <span class="mw-headline" id="Introduction"> Introduction </span></h2>
<p>We have used PCA to reduce the dimension of the data.  There is a closely related
preprocessing step called <b>whitening</b> (or, in some other literatures, <b>sphering</b>)
which is needed for some algorithms.  If we are training on images,
the raw input is redundant, since adjacent pixel values
are highly correlated.  The goal of whitening is to make the input less redundant; more formally,
our desiderata are that our learning algorithms sees a training input where (i) the features are less
correlated with each other, and (ii) the features all have the same variance.
</p>
<h2> <span class="mw-headline" id="2D_example"> 2D example </span></h2>
<p>We will first describe whitening using our previous 2D example.  We will then 
describe how this can be combined with smoothing, and finally how to combine
this with PCA. 
</p><p>How can we make our input features uncorrelated with each other?  We had
already done this when computing <img class="tex" alt="\textstyle x_{\rm rot}^{(i)} = U^Tx^{(i)}" src="/stanford-ufldl/archive/wiki/images/math/c/d/0/cd047246fd68f6d52b2fd068e063c0ef.png"/>.  
Repeating our previous figure, our plot for <img class="tex" alt="\textstyle x_{\rm rot}" src="/stanford-ufldl/archive/wiki/images/math/1/7/0/170047e804738636731477291969d554.png"/> was:
</p><p><a href="" class="image"><img alt="PCA-rotated.png" src="/stanford-ufldl/archive/wiki/images/thumb/1/12/PCA-rotated.png/600px-PCA-rotated.png" width="600" height="450"/></a>
</p><p>The covariance matrix of this data is given by:
</p><p><img class="tex" alt="\begin{align}
\begin{bmatrix}
7.29 &amp; 0  \\
0 &amp; 0.69
\end{bmatrix}.
\end{align}" src="/stanford-ufldl/archive/wiki/images/math/f/e/5/fe5ed797b9c818df5bc8bf5d5c001e02.png"/>
</p><p>(Note: Technically, many of the
statements in this section about the "covariance" will be true only if the data
has zero mean.  In the rest of this section, we will take this assumption as
implicit in our statements.  However, even if the data's mean isn't exactly zero, 
the intuitions we're presenting here still hold true, and so this isn't something
that you should worry about.)
</p><p>It is no accident that the diagonal values are <img class="tex" alt="\textstyle \lambda_1" src="/stanford-ufldl/archive/wiki/images/math/e/1/3/e138a7c8755e6a4400dd6bb08974d139.png"/> and <img class="tex" alt="\textstyle \lambda_2" src="/stanford-ufldl/archive/wiki/images/math/4/1/a/41ab4ee633f1ad3d25809270aedbe566.png"/>.  
Further, 
the off-diagonal entries are zero; thus, 
<img class="tex" alt="\textstyle x_{{\rm rot},1}" src="/stanford-ufldl/archive/wiki/images/math/0/0/6/0066d1e2efa2f0019a3dfd3469862934.png"/> and <img class="tex" alt="\textstyle x_{{\rm rot},2}" src="/stanford-ufldl/archive/wiki/images/math/3/f/2/3f2601aaa1d6e648c789bd9a831cc4eb.png"/> are uncorrelated, satisfying one of our desiderata 
for whitened data (that the features be less correlated).
</p><p>To make each of our input features have unit variance, we can simply rescale
each feature <img class="tex" alt="\textstyle x_{{\rm rot},i}" src="/stanford-ufldl/archive/wiki/images/math/d/1/5/d1527b3272bc5c1fe3fc308c7a21e689.png"/> by <img class="tex" alt="\textstyle 1/\sqrt{\lambda_i}" src="/stanford-ufldl/archive/wiki/images/math/7/a/d/7ad8b4911f758fec9b3c6d0b4b61a82c.png"/>.  Concretely, we define
our whitened data <img class="tex" alt="\textstyle x_{{\rm PCAwhite}} \in \Re^n" src="/stanford-ufldl/archive/wiki/images/math/9/6/9/9693d90272b2475c8369fa23df7267ed.png"/> as follows: 
</p>
<dl><dd><img class="tex" alt="\begin{align}
x_{{\rm PCAwhite},i} = \frac{x_{{\rm rot},i} }{\sqrt{\lambda_i}}.   
\end{align}" src="/stanford-ufldl/archive/wiki/images/math/e/2/9/e296118ba2bdf453dbe38426359f2230.png"/>
</dd></dl>
<p>Plotting <img class="tex" alt="\textstyle x_{{\rm PCAwhite}}" src="/stanford-ufldl/archive/wiki/images/math/a/3/1/a3135c6f5975d0a74cd2d9082be9638a.png"/>, we get:
</p><p><a href="" class="image"><img alt="PCA-whitened.png" src="/stanford-ufldl/archive/wiki/images/thumb/9/98/PCA-whitened.png/600px-PCA-whitened.png" width="600" height="450"/></a>
</p><p>This data now has covariance equal to the identity matrix <img class="tex" alt="\textstyle I" src="/stanford-ufldl/archive/wiki/images/math/5/4/f/54f708ffb9cc17b9820863a36120c90c.png"/>.  We say that
<img class="tex" alt="\textstyle x_{{\rm PCAwhite}}" src="/stanford-ufldl/archive/wiki/images/math/a/3/1/a3135c6f5975d0a74cd2d9082be9638a.png"/> is our <b>PCA whitened</b> version of the data: The 
different components of <img class="tex" alt="\textstyle x_{{\rm PCAwhite}}" src="/stanford-ufldl/archive/wiki/images/math/a/3/1/a3135c6f5975d0a74cd2d9082be9638a.png"/> are uncorrelated and have
unit variance. 
</p><p><b>Whitening combined with dimensionality reduction.</b> 
If you want to have data that is whitened and which is lower dimensional than
the original input, you can also optionally keep only the top <img class="tex" alt="\textstyle k" src="/stanford-ufldl/archive/wiki/images/math/b/0/0/b0066e761791cae480158b649e5f5a69.png"/> components of
<img class="tex" alt="\textstyle x_{{\rm PCAwhite}}" src="/stanford-ufldl/archive/wiki/images/math/a/3/1/a3135c6f5975d0a74cd2d9082be9638a.png"/>.  When we combine PCA whitening with regularization
(described later), the last few components of <img class="tex" alt="\textstyle x_{{\rm PCAwhite}}" src="/stanford-ufldl/archive/wiki/images/math/a/3/1/a3135c6f5975d0a74cd2d9082be9638a.png"/> will be
nearly zero anyway, and thus can safely be dropped.
</p>
<h2> <span class="mw-headline" id="ZCA_Whitening"> ZCA Whitening </span></h2>
<p>Finally, it turns out that this way of getting the 
data to have covariance identity <img class="tex" alt="\textstyle I" src="/stanford-ufldl/archive/wiki/images/math/5/4/f/54f708ffb9cc17b9820863a36120c90c.png"/> isn't unique. 
Concretely, if 
<img class="tex" alt="\textstyle R" src="/stanford-ufldl/archive/wiki/images/math/f/e/e/fee54137ee7748e26642e71145effa05.png"/> is any orthogonal matrix, so that it satisfies <img class="tex" alt="\textstyle RR^T = R^TR = I" src="/stanford-ufldl/archive/wiki/images/math/7/7/d/77d64d6a092c3f7adb9eae6eb4af41ff.png"/> (less formally,
if <img class="tex" alt="\textstyle R" src="/stanford-ufldl/archive/wiki/images/math/f/e/e/fee54137ee7748e26642e71145effa05.png"/> is a rotation/reflection matrix),
then <img class="tex" alt="\textstyle R \,x_{\rm PCAwhite}" src="/stanford-ufldl/archive/wiki/images/math/b/c/d/bcd43a98b71d807cddbdb7a3a33bbc1a.png"/> will also have identity covariance.  
In <b>ZCA whitening</b>,
we choose <img class="tex" alt="\textstyle R = U" src="/stanford-ufldl/archive/wiki/images/math/b/6/1/b61977ba8ab2bacb0c31fa5575db43fd.png"/>.  We define 
</p>
<dl><dd><img class="tex" alt="\begin{align}
x_{\rm ZCAwhite} = U x_{\rm PCAwhite}
\end{align}" src="/stanford-ufldl/archive/wiki/images/math/c/f/b/cfb1fa6b1049a5fdb2da4d7e88856751.png"/>
</dd></dl>
<p>Plotting <img class="tex" alt="\textstyle x_{\rm ZCAwhite}" src="/stanford-ufldl/archive/wiki/images/math/a/6/6/a668553308d25ae0f796a9f92c807931.png"/>, we get: 
</p><p><a href="" class="image"><img alt="ZCA-whitened.png" src="/stanford-ufldl/archive/wiki/images/thumb/a/a4/ZCA-whitened.png/600px-ZCA-whitened.png" width="600" height="450"/></a>
</p><p>It can be shown that out of all possible choices for <img class="tex" alt="\textstyle R" src="/stanford-ufldl/archive/wiki/images/math/f/e/e/fee54137ee7748e26642e71145effa05.png"/>, 
this choice of rotation causes <img class="tex" alt="\textstyle x_{\rm ZCAwhite}" src="/stanford-ufldl/archive/wiki/images/math/a/6/6/a668553308d25ae0f796a9f92c807931.png"/> to be as close as possible to the 
original input data <img class="tex" alt="\textstyle x" src="/stanford-ufldl/archive/wiki/images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png"/>.  
</p><p>When using ZCA whitening (unlike PCA whitening), we usually keep all <img class="tex" alt="\textstyle n" src="/stanford-ufldl/archive/wiki/images/math/0/c/5/0c59de0fa75c1baa1c024aabfa43b2e3.png"/> dimensions
of the data, and do not try to reduce its dimension.
</p>
<h2> <span class="mw-headline" id="Regularizaton"> Regularizaton </span></h2>
<p>When implementing PCA whitening or ZCA whitening in practice, sometimes some
of the eigenvalues <img class="tex" alt="\textstyle \lambda_i" src="/stanford-ufldl/archive/wiki/images/math/2/3/5/23536ce45f0ee57fffa389163f8437bd.png"/> will be numerically close to 0, and thus the scaling
step where we divide by <img class="tex" alt="\sqrt{\lambda_i}" src="/stanford-ufldl/archive/wiki/images/math/3/e/8/3e85dc0c50d11861f9d02bb43ab2d989.png"/> would involve dividing by a value close to zero; this 
may cause the data to blow up (take on large values) or otherwise be numerically unstable.  In practice, we 
therefore implement this scaling step using 
a small amount of regularization, and add a small constant <img class="tex" alt="\textstyle \epsilon" src="/stanford-ufldl/archive/wiki/images/math/a/8/e/a8eae7b5e90c024c40de690158e0e6b1.png"/> 
to the eigenvalues before taking their square root and inverse:
</p>
<dl><dd><img class="tex" alt="\begin{align}
x_{{\rm PCAwhite},i} = \frac{x_{{\rm rot},i} }{\sqrt{\lambda_i + \epsilon}}.
\end{align}" src="/stanford-ufldl/archive/wiki/images/math/6/7/b/67be9aaa628b437297c08a916d0d5307.png"/>
</dd></dl>
<p>When <img class="tex" alt="\textstyle x" src="/stanford-ufldl/archive/wiki/images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png"/> takes values around <img class="tex" alt="\textstyle [-1,1]" src="/stanford-ufldl/archive/wiki/images/math/8/5/a/85a1c5a07f21a9eebbfb1dca380f8d38.png"/>, a value of <img class="tex" alt="\textstyle \epsilon \approx 10^{-5}" src="/stanford-ufldl/archive/wiki/images/math/c/d/d/cdd6f0cc52395a161edf391fad0ef2ef.png"/>
might be typical. 
</p><p>For the case of images, adding <img class="tex" alt="\textstyle \epsilon" src="/stanford-ufldl/archive/wiki/images/math/a/8/e/a8eae7b5e90c024c40de690158e0e6b1.png"/> here also has the effect of slightly smoothing (or low-pass
filtering) the input image.  This also has a desirable effect of removing aliasing artifacts
caused by the way pixels are laid out in an image, and can improve the features learned 
(details are beyond the scope of these notes). 
</p><p>ZCA whitening is a form of pre-processing of the data that maps it from <img class="tex" alt="\textstyle x" src="/stanford-ufldl/archive/wiki/images/math/f/6/c/f6c0f8758a1eb9c99c0bbe309ff2c5a5.png"/> to
<img class="tex" alt="\textstyle x_{\rm ZCAwhite}" src="/stanford-ufldl/archive/wiki/images/math/a/6/6/a668553308d25ae0f796a9f92c807931.png"/>.  It turns out that this is also a rough model of how the
biological eye (the retina) processes images.  Specifically, as your eye
perceives images, most adjacent "pixels" in your eye will perceive very
similar values, since adjacent parts of an image tend to be highly correlated
in intensity.  It is thus wasteful for your eye to have to transmit every pixel
separately (via your optic nerve) to your brain.  Instead, your retina performs
a decorrelation operation (this is done via retinal neurons that compute a function
called "on center, off surround/off center, on surround") which is similar to that
performed by ZCA.  This results in a less redundant representation of the input
image, which is then transmitted to your brain.
</p><p><br/>
</p>
<div style="text-align: center;font-size:small;background-color: #eeeeee; border-style: solid; border-width: 1px; padding: 5px">
<p><a href="/stanford-ufldl/archive/wiki/PCA" title="PCA">PCA</a> | <strong class="selflink">Whitening</strong> | <a href="/stanford-ufldl/archive/wiki/Implementing_PCA/Whitening" title="Implementing PCA/Whitening">Implementing PCA/Whitening</a> | <a href="/stanford-ufldl/archive/wiki/Exercise_PCA_in_2D" title="Exercise:PCA in 2D">Exercise:PCA in 2D</a> | <a href="/stanford-ufldl/archive/wiki/Exercise_PCA_and_Whitening" title="Exercise:PCA and Whitening">Exercise:PCA and Whitening</a>
</p>
</div>
<p><br/>
</p>
<div style="text-align: left;font-size:small;background-color: #eeeeee; border-style: solid; border-width: 1px; padding: 5px">
<p>Language&nbsp;: <a href="/stanford-ufldl/archive/wiki/%E7%99%BD%E5%8C%96" title="白化">中文</a>
</p>
</div>

<!-- 
NewPP limit report
Preprocessor node count: 324/1000000
Post-expand include size: 412/2097152 bytes
Template argument size: 12/2097152 bytes
Expensive parser function count: 0/100
-->

<div class="printfooter">
</div>		<div id="catlinks" class="catlinks catlinks-allhidden"></div>		<!-- end content -->
				<div class="visualClear"></div>
	</div>
</div></div>
<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
				 <li id="ca-nstab-main" class="selected"><a href="/stanford-ufldl/archive/wiki/Whitening" title="View the content page [c]" accesskey="c">Page</a></li>
				 <li id="ca-talk" class="new"><a href="" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-viewsource"><a href="" title="This page is protected.
You can view its source [e]" accesskey="e">View source</a></li>
				 <li id="ca-history"><a href="" title="Past revisions of this page [h]" accesskey="h">History</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(/stanford-ufldl/archive/wiki/skins/common/images/dolphin-openclipart.png);" href="/stanford-ufldl/archive/wiki/Main_Page" title="Visit the main page"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class="generated-sidebar portlet" id="p-ufldl_resources">
		<h5>ufldl resources</h5>
		<div class="pBody">
			<ul>
				<li id="n-UFLDL-Tutorial"><a href="/stanford-ufldl/archive/wiki/UFLDL_Tutorial">UFLDL Tutorial</a></li>
				<li id="n-Recommended-Readings"><a href="/stanford-ufldl/archive/wiki/UFLDL_Recommended_Readings">Recommended Readings</a></li>
			</ul>
		</div>
	</div>
	<div class="generated-sidebar portlet" id="p-wiki">
		<h5>wiki</h5>
		<div class="pBody">
			<ul>
				<li id="n-mainpage-description"><a href="/stanford-ufldl/archive/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-recentchanges"><a href="" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-randompage"><a href="" title="Load a random page [x]" accesskey="x">Random page</a></li>
				<li id="n-help"><a href="" title="The place to find out">Help</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="" id="searchform">
				<input type="hidden" name="title" value="Special:Search" disabled/>
				<input id="searchInput" title="Search Ufldl" accesskey="f" type="search" name="search" disabled/>
				<input type="submit" name="go" class="searchButton" id="searchGoButton" value="Go" title="Go to a page with this exact name if exists" disabled/>&nbsp;
				<input type="submit" name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search the pages for this text" disabled/>
			</form>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="" title="List of all wiki pages that link here [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-specialpages"><a href="" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="" title="Permanent link to this revision of the page">Permanent link</a></li>			</ul>
		</div>
	</div>
</div><!-- end of the left (by default at least) column -->
<div class="visualClear"></div>
<div id="footer">
	<ul id="f-list">
		<li id="lastmod"> This page was last modified on 7 April 2013, at 13:20.</li>
		<li id="privacy"><a href="/stanford-ufldl/archive/wiki/Ufldl_Privacy_policy" title="Ufldl:Privacy policy">Privacy policy</a></li>
		<li id="about"><a href="/stanford-ufldl/archive/wiki/Ufldl_About" title="Ufldl:About">About Ufldl</a></li>
		<li id="disclaimer"><a href="/stanford-ufldl/archive/wiki/Ufldl_General_disclaimer" title="Ufldl:General disclaimer">Disclaimers</a></li>
	</ul>
</div>
</div>

<!-- Served in 0.108 secs. -->
</body>
</html>
