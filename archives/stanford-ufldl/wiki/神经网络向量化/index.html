
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>神经网络向量化 - Ufldl</title>
<link rel="stylesheet" href="/stanford-ufldl/archive/wiki/skins/common/shared.css" media="screen"/>
<link rel="stylesheet" href="/stanford-ufldl/archive/wiki/skins/monobook/main.css" media="screen"/>

</head>

<body class="mediawiki ltr ns-0 ns-subject page-神经网络向量化 skin-monobook">
<div id="globalWrapper">
<div id="column-content"><div id="content">
	<a id="top"></a>
	
	<h1 id="firstHeading" class="firstHeading">神经网络向量化</h1>
	<div id="bodyContent">
		<h3 id="siteSub">From Ufldl</h3>
		<div id="contentSub"></div>
		<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>
<p>在本节，我们将引入神经网络的向量化版本。在前面关于<a href="/stanford-ufldl/archive/wiki/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" title="神经网络">神经网络</a>介绍的章节中，我们已经给出了一个部分向量化的实现，它在一次输入一个训练样本时是非常有效率的。下边我们看看如何实现同时处理多个训练样本的算法。具体来讲，我们将把正向传播、反向传播这两个步骤以及稀疏特征集学习扩展为多训练样本版本。
</p><p><br/>
</p>
<table id="toc" class="toc"><tr><td><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#.E6.AD.A3.E5.90.91.E4.BC.A0.E6.92.AD"><span class="tocnumber">1</span> <span class="toctext">正向传播</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#.E5.8F.8D.E5.90.91.E4.BC.A0.E6.92.AD"><span class="tocnumber">2</span> <span class="toctext">反向传播</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#.E7.A8.80.E7.96.8F.E8.87.AA.E7.BC.96.E7.A0.81.E7.BD.91.E7.BB.9C"><span class="tocnumber">3</span> <span class="toctext">稀疏自编码网络</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#.E4.B8.AD.E8.8B.B1.E6.96.87.E5.AF.B9.E7.85.A7"><span class="tocnumber">4</span> <span class="toctext">中英文对照</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#.E4.B8.AD.E6.96.87.E8.AF.91.E8.80.85"><span class="tocnumber">5</span> <span class="toctext">中文译者</span></a></li>
</ul>
</td></tr></table><script>if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } </script>
<h2> <span class="mw-headline" id=".E6.AD.A3.E5.90.91.E4.BC.A0.E6.92.AD">正向传播</span></h2>
<p>考虑一个三层网络(一个输入层、一个隐含层、以及一个输出层)，并且假定x是包含一个单一训练样本<img class="tex" alt="x^{(i)} \in \Re^{n}" src="/stanford-ufldl/archive/wiki/images/math/7/5/6/75642aa64cbeaac87299d8950ef9e1e3.png"/> 的列向量。则向量化的正向传播步骤如下：
</p><p><br/>
</p>
<dl><dd><img class="tex" alt="\begin{align}
z^{(2)} &amp;= W^{(1)} x + b^{(1)} \\
a^{(2)} &amp;= f(z^{(2)}) \\
z^{(3)} &amp;= W^{(2)} a^{(2)} + b^{(2)} \\
h_{W,b}(x) &amp;= a^{(3)} = f(z^{(3)})
\end{align}" src="/stanford-ufldl/archive/wiki/images/math/9/6/9/9690acc03c1e5133b0509257b532b4f7.png"/>
</dd></dl>
<p><br/>
这对于单一训练样本而言是非常有效的一种实现，但是当我们需要处理<tt>m</tt>个训练样本时，则需要把如上步骤放入一个<tt>for</tt>循环中。
</p><p><br/>
更具体点来说，参照逻辑回归向量化的例子，我们用Matlab/Octave风格变量<tt>x</tt>表示包含输入训练样本的矩阵，<tt>x(:,i)</tt>代表第<img class="tex" alt="\textstyle i" src="/stanford-ufldl/archive/wiki/images/math/0/b/3/0b36ee693126b34b58f77dba7ed23987.png"/>个训练样本。则x正向传播步骤可如下实现：
</p><p><br/>
</p>
<div dir="ltr" class="mw-geshi" style="text-align: left;"><div class="matlab source-matlab"><pre class="de1"><span class="co1">% 非向量化实现</span>
<span class="kw1">for</span> <span class="kw2"><span class="re0">i</span></span>=<span class="nu0">1</span>:m, 
  z2 = W1 * x<span class="br0">&#40;</span>:,<span class="kw2"><span class="re0">i</span></span><span class="br0">&#41;</span> + b1;
  a2 = f<span class="br0">&#40;</span>z2<span class="br0">&#41;</span>;
  z3 = W2 * a2 + b2;
  h<span class="br0">&#40;</span>:,<span class="kw2"><span class="re0">i</span></span><span class="br0">&#41;</span> = f<span class="br0">&#40;</span>z3<span class="br0">&#41;</span>;
<span class="kw1">end</span>;</pre></div></div> 
<p><br/>
这个<tt>for</tt>循环能否去掉呢？对于很多算法而言，我们使用向量来表示计算过程中的中间结果。例如在前面的非向量化实现中，<tt>z2</tt>,<tt>a2</tt>,<tt>z3</tt>都是列向量，分别用来计算隐层和输出层的激励结果。为了充分利用并行化和高效矩阵运算的优势，我们希望算法能同时处理多个训练样本。让我们先暂时忽略前面公式中的<tt>b1</tt>和<tt>b2</tt>(把它们设置为0)，那么可以实现如下:
</p><p><br/>
</p>
<div dir="ltr" class="mw-geshi" style="text-align: left;"><div class="matlab source-matlab"><pre class="de1"><span class="co1">% 向量化实现 (忽略 b1, b2)</span>
z2 = W1 * x;
a2 = f<span class="br0">&#40;</span>z2<span class="br0">&#41;</span>;
z3 = W2 * a2;
h = f<span class="br0">&#40;</span>z3<span class="br0">&#41;</span></pre></div></div> 
<p><br/>
在这个实现中，<tt>z2</tt>,<tt>a2</tt>,<tt>z3</tt>都是矩阵，每个训练样本对应矩阵的一列。在对多个训练样本实现向量化时常用的设计模式是，虽然前面每个样本对应一个列向量（比如<tt>z2</tt>），但我们可把这些列向量堆叠成一个矩阵以充分享受矩阵运算带来的好处。这样，在这个例子中，<tt>a2</tt>就成了一个<span class="texhtml"><i>s</i><sub>2</sub></span> X <span class="texhtml"><i>m</i></span>的矩阵(<span class="texhtml"><i>s</i><sub>2</sub></span>是网络第二层中的神经元数，<span class="texhtml"><i>m</i></span>是训练样本个数)。矩阵<tt>a2</tt>的物理含义是，当第<span class="texhtml"><i>i</i></span>个训练样本<tt>x(:i)</tt>输入到网络中时，它的第<span class="texhtml"><i>i</i></span>列就表示这个输入信号对隐神经元 (网络第二层)的激励结果。
</p><p><br/>
在上面的实现中，我们假定激活函数<tt>f(z)</tt>接受矩阵形式的输入<tt>z</tt>，并对输入矩阵按列分别施以激活函数。需要注意的是，你在实现<tt>f(z)</tt>的时候要尽量多用Matlab/Octave的矩阵操作，并尽量避免使用for循环。假定激活函数采用Sigmoid函数，则实现代码如下所示:
</p><p><br/>
</p>
<div dir="ltr" class="mw-geshi" style="text-align: left;"><div class="matlab source-matlab"><pre class="de1"><span class="co1">% 低效的、非向量化的激活函数实现</span>
<span class="kw1">function</span> output = unvectorized_f<span class="br0">&#40;</span>z<span class="br0">&#41;</span>
output = <span class="kw2">zeros</span><span class="br0">&#40;</span><span class="kw2">size</span><span class="br0">&#40;</span>z<span class="br0">&#41;</span><span class="br0">&#41;</span>
<span class="kw1">for</span> <span class="kw2"><span class="re0">i</span></span>=<span class="nu0">1</span>:<span class="kw2">size</span><span class="br0">&#40;</span>z,<span class="nu0">1</span><span class="br0">&#41;</span>, 
  <span class="kw1">for</span> <span class="kw2"><span class="re0">j</span></span>=<span class="nu0">1</span>:<span class="kw2">size</span><span class="br0">&#40;</span>z,<span class="nu0">2</span><span class="br0">&#41;</span>,
    output<span class="br0">&#40;</span><span class="kw2"><span class="re0">i</span></span>,<span class="kw2"><span class="re0">j</span></span><span class="br0">&#41;</span> = <span class="nu0">1</span>/<span class="br0">&#40;</span><span class="nu0">1</span>+<span class="kw2">exp</span><span class="br0">&#40;</span>-z<span class="br0">&#40;</span><span class="kw2"><span class="re0">i</span></span>,<span class="kw2"><span class="re0">j</span></span><span class="br0">&#41;</span><span class="br0">&#41;</span><span class="br0">&#41;</span>;
  <span class="kw1">end</span>; 
<span class="kw1">end</span>;
<span class="kw1">end</span>
&nbsp;
&nbsp;
<span class="co1">% 高效的、向量化激活函数实现</span>
<span class="kw1">function</span> output = vectorized_f<span class="br0">&#40;</span>z<span class="br0">&#41;</span>
output = <span class="nu0">1</span>./<span class="br0">&#40;</span><span class="nu0">1</span>+<span class="kw2">exp</span><span class="br0">&#40;</span>-z<span class="br0">&#41;</span><span class="br0">&#41;</span>;     <span class="co1">% &quot;./&quot; 在Matlab或Octave中表示对矩阵的每个元素分别进行除法操作</span>
<span class="kw1">end</span></pre></div></div>
<p><br/>
最后，我们上面的正向传播向量化实现中忽略了<tt>b1</tt>和<tt>b2</tt>，现在要把他们包含进来，为此我们需要用到Matlab/Octave的内建函数<tt>repmat</tt>：
</p><p><br/>
</p>
<div dir="ltr" class="mw-geshi" style="text-align: left;"><div class="matlab source-matlab"><pre class="de1"><span class="co1">% 正向传播的向量化实现</span>
z2 = W1 * x + <span class="kw2">repmat</span><span class="br0">&#40;</span>b1,<span class="nu0">1</span>,m<span class="br0">&#41;</span>;
a2 = f<span class="br0">&#40;</span>z2<span class="br0">&#41;</span>;
z3 = W2 * a2 + <span class="kw2">repmat</span><span class="br0">&#40;</span>b2,<span class="nu0">1</span>,m<span class="br0">&#41;</span>;
h = f<span class="br0">&#40;</span>z3<span class="br0">&#41;</span></pre></div></div>
<p><br/>
<tt>repmat(b1,1,m)</tt>的运算效果是，它把列向量<tt>b1</tt>拷贝<span class="texhtml"><i>m</i></span>份，然后堆叠成如下矩阵：
</p><p><br/>
</p>
<dl><dd><dl><dd><img class="tex" alt="
\begin{bmatrix}
| &amp; | &amp;  &amp; |  \\
{\rm b1}  &amp; {\rm b1}  &amp; \cdots &amp; {\rm b1} \\
| &amp; | &amp;  &amp; |  
\end{bmatrix}.
" src="/stanford-ufldl/archive/wiki/images/math/7/b/a/7ba1557030d1461b87b462be4ed864ac.png"/>
</dd></dl>
</dd></dl>
<p><br/>
这就构成一个<span class="texhtml"><i>s</i><sub>2</sub></span> X <span class="texhtml"><i>m</i></span>的矩阵。它和<tt>W1 * x</tt>相加，就等于是把<tt>W1 * x</tt>矩阵（译者注：这里<tt>x</tt>是训练矩阵而非向量, 所以<tt>W1 * x</tt>代表两个矩阵相乘，结果还是一个矩阵）的每一列加上<tt>b1</tt>。如果不熟悉的话，可以参考Matlab/Octave的帮助文档获取更多信息(输入“<tt>help repmat</tt>”)。<tt>rampat</tt>作为Matlab/Octave的内建函数，运行起来是相当高效的，远远快过我们自己用<tt>for</tt>循环实现的效果。
</p><p><br/>
</p>
<h2> <span class="mw-headline" id=".E5.8F.8D.E5.90.91.E4.BC.A0.E6.92.AD">反向传播</span></h2>
<p>现在我们来描述反向传播向量化的思路。在阅读这一节之前，强烈建议各位仔细阅读前面介绍的正向传播的例子代码，确保你已经完全理解。下边我们只会给出反向传播向量化实现的大致纲要，而由你来完成具体细节的推导（见<a href="/stanford-ufldl/archive/wiki/Exercise_Vectorization" title="Exercise:Vectorization">向量化练习</a>）。
</p><p><br/>
对于监督学习，我们有一个包含<span class="texhtml"><i>m</i></span>个带类别标号样本的训练集<img class="tex" alt="\{ (x^{(1)}, y^{(1)}), \ldots, (x^{(m)}, y^{(m)}) \}" src="/stanford-ufldl/archive/wiki/images/math/5/e/c/5ec89e9cf3712d45b80e93258352ea8f.png"/>。
(对于自编码网络，我们只需令<span class="texhtml"><i>y</i><sup>(<i>i</i>)</sup> = <i>x</i><sup>(<i>i</i>)</sup></span>即可,  但这里考虑的是更一般的情况。)
</p><p><br/>
假定网络的输出有<span class="texhtml"><i>s</i><sub>3</sub></span>维，因而每个样本的类别标号向量就记为<img class="tex" alt="y^{(i)} \in \Re^{s_3}" src="/stanford-ufldl/archive/wiki/images/math/e/d/f/edfdd985de42e6c6724064dfa79499e2.png"/>。在我们的Matlab/Octave数据结构实现中，把这些输出按列合在一起形成一个Matlab/Octave风格变量<tt>y</tt>，其中第<tt>i</tt>列<tt>y(:,i)</tt>就是<span class="texhtml"><i>y</i><sup>(<i>i</i>)</sup></span>。
</p><p><br/>
现在我们要计算梯度项<img class="tex" alt="\nabla_{W^{(l)}} J(W,b)" src="/stanford-ufldl/archive/wiki/images/math/a/0/4/a048bebd8683f9762667261147708bdb.png"/>和<img class="tex" alt="\nabla_{b^{(l)}} J(W,b)" src="/stanford-ufldl/archive/wiki/images/math/7/0/7/70718292fd6a578aae46a709a4c72030.png"/>。对于梯度中的第一项，就像过去在反向传播算法中所描述的那样，对于每个训练样本<span class="texhtml">(<i>x</i>,<i>y</i>)</span>，我们可以这样来计算：
</p><p><br/>
</p>
<dl><dd><dl><dd><img class="tex" alt="
\begin{align}
\delta^{(3)} &amp;= - (y - a^{(3)}) \bullet f'(z^{(3)}), \\
\delta^{(2)} &amp;= ((W^{(2)})^T\delta^{(3)}) \bullet f'(z^{(2)}), \\
\nabla_{W^{(2)}} J(W,b;x,y) &amp;= \delta^{(3)} (a^{(2)})^T, \\
\nabla_{W^{(1)}} J(W,b;x,y) &amp;= \delta^{(2)} (a^{(1)})^T. 
\end{align} 
" src="/stanford-ufldl/archive/wiki/images/math/4/7/6/4761878cb051fffcff5d159a49e0163e.png"/>
</dd></dl>
</dd></dl>
<p><br/>
在这里<img class="tex" alt="\bullet" src="/stanford-ufldl/archive/wiki/images/math/b/f/5/bf588c17a2f1ba670dd67abd8ef6b8c6.png"/>表示对两个向量按对应元素相乘的运算（译者注：其结果还是一个向量）。为了描述简单起见，我们这里暂时忽略对参数<span class="texhtml"><i>b</i><sup>(<i>l</i>)</sup></span>的求导，不过在你真正实现反向传播时，还是需要计算关于它们的导数的。
</p><p><br/>
假定我们已经实现了向量化的正向传播方法，如前面那样计算了矩阵形式的变量<tt>z2</tt>, <tt>a2</tt>, <tt>z3</tt>和<tt>h</tt>，那么反向传播的非向量化版本可如下实现：
</p><p><br/>
</p>
<div dir="ltr" class="mw-geshi" style="text-align: left;"><div class="matlab source-matlab"><pre class="de1">gradW1 = <span class="kw2">zeros</span><span class="br0">&#40;</span><span class="kw2">size</span><span class="br0">&#40;</span>W1<span class="br0">&#41;</span><span class="br0">&#41;</span>;
gradW2 = <span class="kw2">zeros</span><span class="br0">&#40;</span><span class="kw2">size</span><span class="br0">&#40;</span>W2<span class="br0">&#41;</span><span class="br0">&#41;</span>; 
<span class="kw1">for</span> <span class="kw2"><span class="re0">i</span></span>=<span class="nu0">1</span>:m,
  delta3 = -<span class="br0">&#40;</span>y<span class="br0">&#40;</span>:,<span class="kw2"><span class="re0">i</span></span><span class="br0">&#41;</span> - h<span class="br0">&#40;</span>:,<span class="kw2"><span class="re0">i</span></span><span class="br0">&#41;</span><span class="br0">&#41;</span> .* fprime<span class="br0">&#40;</span>z3<span class="br0">&#40;</span>:,<span class="kw2"><span class="re0">i</span></span><span class="br0">&#41;</span><span class="br0">&#41;</span>; 
  delta2 = W2'*delta3<span class="br0">&#40;</span>:,<span class="kw2"><span class="re0">i</span></span><span class="br0">&#41;</span> .* fprime<span class="br0">&#40;</span>z2<span class="br0">&#40;</span>:,<span class="kw2"><span class="re0">i</span></span><span class="br0">&#41;</span><span class="br0">&#41;</span>;
&nbsp;
  gradW2 = gradW2 + delta3*a2<span class="br0">&#40;</span>:,<span class="kw2"><span class="re0">i</span></span><span class="br0">&#41;</span>';
  gradW1 = gradW1 + delta2*a1<span class="br0">&#40;</span>:,<span class="kw2"><span class="re0">i</span></span><span class="br0">&#41;</span>'; 
<span class="kw1">end</span>;</pre></div></div> 
<p><br/>
在这个实现中，有一个<tt>for</tt>循环。而我们想要一个能同时处理所有样本、且去除这个<tt>for</tt>循环的向量化版本。
</p><p><br/>
为做到这一点，我们先把向量<tt>delta3</tt>和<tt>delta2</tt>替换为矩阵，其中每列对应一个训练样本。我们还要实现一个函数<tt>fprime(z)</tt>，该函数接受矩阵形式的输入<tt>z</tt>，并且对矩阵的按元素分别执行<img class="tex" alt="f'(\cdot)" src="/stanford-ufldl/archive/wiki/images/math/f/f/6/ff62381ad386ec3826477d743df34b6c.png"/>。这样，上面<tt>for</tt>循环中的4行Matlab代码中每行都可单独向量化，以一行新的（向量化的）Matlab代码替换它（不再需要外层的<tt>for</tt>循环）。
</p><p><br/>
在<a href="/stanford-ufldl/archive/wiki/Exercise_Vectorization" title="Exercise:Vectorization">向量化练习</a>中，我们要求你自己去推导出这个算法的向量化版本。如果你已经能从上面的描述中了解如何去做，那么我们强烈建议你去实践一下。虽然我们已经为你准备了<a href="/stanford-ufldl/archive/wiki/Backpropagation_vectorization_hints" title="Backpropagation vectorization hints">反向传播的向量化实现提示</a>，但还是鼓励你在不看提示的情况下自己去推导一下。
</p><p><br/>
</p>
<h2> <span class="mw-headline" id=".E7.A8.80.E7.96.8F.E8.87.AA.E7.BC.96.E7.A0.81.E7.BD.91.E7.BB.9C">稀疏自编码网络</span></h2>
<p><a href="/stanford-ufldl/archive/wiki/%E8%87%AA%E7%BC%96%E7%A0%81%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%80%E7%96%8F%E6%80%A7" title="自编码算法与稀疏性">稀疏自编码</a>网络中包含一个额外的稀疏惩罚项，目的是限制神经元的平均激活率，使其接近某个（预设的）目标激活率<span class="texhtml">&rho;</span>。其实在对单个训练样本上执行反向传播时，我们已经考虑了如何计算这个稀疏惩罚项，如下所示：
</p><p><br/>
</p>
<dl><dd><img class="tex" alt="\begin{align}
\delta^{(2)}_i =
  \left( \left( \sum_{j=1}^{s_{2}} W^{(2)}_{ji} \delta^{(3)}_j \right)
+ \beta \left( - \frac{\rho}{\hat\rho_i} + \frac{1-\rho}{1-\hat\rho_i} \right) \right) f'(z^{(2)}_i) .
\end{align}" src="/stanford-ufldl/archive/wiki/images/math/b/e/f/bef8a29947bfb0d746c54a7d922874e8.png"/>
</dd></dl>
<p><br/>
在非向量化的实现中，计算代码如下：
</p><p><br/>
</p>
<div dir="ltr" class="mw-geshi" style="text-align: left;"><div class="matlab source-matlab"><pre class="de1"><span class="co1">% 稀疏惩罚Delta</span>
sparsity_delta = - rho ./ rho_hat + <span class="br0">&#40;</span><span class="nu0">1</span> - rho<span class="br0">&#41;</span> ./ <span class="br0">&#40;</span><span class="nu0">1</span> - rho_hat<span class="br0">&#41;</span>;
<span class="kw1">for</span> <span class="kw2"><span class="re0">i</span></span>=<span class="nu0">1</span>:m,
  <span class="sy0">...</span>
  <span class="me1">delta2</span> = <span class="br0">&#40;</span>W2'*delta3<span class="br0">&#40;</span>:,<span class="kw2"><span class="re0">i</span></span><span class="br0">&#41;</span> + <span class="kw2">beta</span>*sparsity_delta<span class="br0">&#41;</span>.* fprime<span class="br0">&#40;</span>z2<span class="br0">&#40;</span>:,<span class="kw2"><span class="re0">i</span></span><span class="br0">&#41;</span><span class="br0">&#41;</span>; 
  <span class="sy0">...</span>
<span class="kw1">end</span>;</pre></div></div>
<p><br/>
但在上面的代码中，仍旧含有一个需要在整个训练集上运行的<tt>for</tt>循环，这里<tt>delta2</tt>是一个列向量。
</p><p><br/>
作为对照，回想一下在向量化的情况下，<tt>delta2</tt>现在应该是一个有m列的矩阵，分别对应着<span class="texhtml"><i>m</i></span>个训练样本。还要注意，稀疏惩罚项<tt>sparsity_delta</tt>对所有的训练样本一视同仁。这意味着要向量化实现上面的计算，只需在构造<tt>delta2</tt>时，往矩阵的每一列上分别加上相同的值即可。因此，要向量化上面的代码，我们只需简单的用<tt>repmat</tt>命令把<tt>sparsity_delta</tt>加到<tt>delta2</tt>的每一列上即可（译者注：这里原文描述得不是很清楚，看似应加到上面代码中<tt>delta2</tt>行等号右边第一项，即<tt>W2'*delta3</tt>上）。
</p><p><br/>
</p>
<h2> <span class="mw-headline" id=".E4.B8.AD.E8.8B.B1.E6.96.87.E5.AF.B9.E7.85.A7">中英文对照</span></h2>
<dl><dd>向量化	        vectorization
</dd><dd>正向传播	forward propagation
</dd><dd>反向传播	backpropagation
</dd><dd>训练样本	training examples
</dd><dd>激活函数	activation function
</dd><dd>稀疏自编码网络	sparse autoencoder
</dd><dd>稀疏惩罚	sparsity penalty
</dd><dd>平均激活率     average firing rate
</dd></dl>
<h2> <span class="mw-headline" id=".E4.B8.AD.E6.96.87.E8.AF.91.E8.80.85">中文译者</span></h2>
<p>阎志涛（zhitao.yan@gmail.com）, 谭晓阳（x.tan@nuaa.edu.cn）, 邓亚峰（dengyafeng@gmail.com）
</p><p><br/>
</p>
<div style="text-align: center;font-size:small;background-color: #eeeeee; border-style: solid; border-width: 1px; padding: 5px">
<p><a href="/stanford-ufldl/archive/wiki/%E7%9F%A2%E9%87%8F%E5%8C%96%E7%BC%96%E7%A8%8B" title="矢量化编程">矢量化编程</a> | <a href="/stanford-ufldl/archive/wiki/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%90%91%E9%87%8F%E5%8C%96%E5%AE%9E%E7%8E%B0%E6%A0%B7%E4%BE%8B" title="逻辑回归的向量化实现样例">逻辑回归的向量化实现样例</a> | <strong class="selflink">神经网络向量化</strong> | <a href="/stanford-ufldl/archive/wiki/Exercise_Vectorization" title="Exercise:Vectorization">Exercise:Vectorization</a>
</p>
</div>
<p><br/>
</p>
<div style="text-align: left;font-size:small;background-color: #eeeeee; border-style: solid; border-width: 1px; padding: 5px">
<p>Language&nbsp;: <a href="/stanford-ufldl/archive/wiki/Neural_Network_Vectorization" title="Neural Network Vectorization">English</a>
</p>
</div>

<!-- 
NewPP limit report
Preprocessor node count: 344/1000000
Post-expand include size: 440/2097152 bytes
Template argument size: 35/2097152 bytes
Expensive parser function count: 0/100
-->

<div class="printfooter">
</div>		<div id="catlinks" class="catlinks catlinks-allhidden"></div>		<!-- end content -->
				<div class="visualClear"></div>
	</div>
</div></div>
<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
				 <li id="ca-nstab-main" class="selected"><a href="/stanford-ufldl/archive/wiki/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%90%91%E9%87%8F%E5%8C%96" title="View the content page [c]" accesskey="c">Page</a></li>
				 <li id="ca-talk" class="new"><a href="" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-viewsource"><a href="" title="This page is protected.
You can view its source [e]" accesskey="e">View source</a></li>
				 <li id="ca-history"><a href="" title="Past revisions of this page [h]" accesskey="h">History</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(/stanford-ufldl/archive/wiki/skins/common/images/dolphin-openclipart.png);" href="/stanford-ufldl/archive/wiki/Main_Page" title="Visit the main page"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class="generated-sidebar portlet" id="p-ufldl_resources">
		<h5>ufldl resources</h5>
		<div class="pBody">
			<ul>
				<li id="n-UFLDL-Tutorial"><a href="/stanford-ufldl/archive/wiki/UFLDL_Tutorial">UFLDL Tutorial</a></li>
				<li id="n-Recommended-Readings"><a href="/stanford-ufldl/archive/wiki/UFLDL_Recommended_Readings">Recommended Readings</a></li>
			</ul>
		</div>
	</div>
	<div class="generated-sidebar portlet" id="p-wiki">
		<h5>wiki</h5>
		<div class="pBody">
			<ul>
				<li id="n-mainpage-description"><a href="/stanford-ufldl/archive/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li>
				<li id="n-recentchanges"><a href="" title="The list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li>
				<li id="n-randompage"><a href="" title="Load a random page [x]" accesskey="x">Random page</a></li>
				<li id="n-help"><a href="" title="The place to find out">Help</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="" id="searchform">
				<input type="hidden" name="title" value="Special:Search" disabled/>
				<input id="searchInput" title="Search Ufldl" accesskey="f" type="search" name="search" disabled/>
				<input type="submit" name="go" class="searchButton" id="searchGoButton" value="Go" title="Go to a page with this exact name if exists" disabled/>&nbsp;
				<input type="submit" name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search the pages for this text" disabled/>
			</form>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="" title="List of all wiki pages that link here [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-specialpages"><a href="" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="" rel="alternate" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="" title="Permanent link to this revision of the page">Permanent link</a></li>			</ul>
		</div>
	</div>
</div><!-- end of the left (by default at least) column -->
<div class="visualClear"></div>
<div id="footer">
	<ul id="f-list">
		<li id="lastmod"> This page was last modified on 8 April 2013, at 05:02.</li>
		<li id="privacy"><a href="/stanford-ufldl/archive/wiki/Ufldl_Privacy_policy" title="Ufldl:Privacy policy">Privacy policy</a></li>
		<li id="about"><a href="/stanford-ufldl/archive/wiki/Ufldl_About" title="Ufldl:About">About Ufldl</a></li>
		<li id="disclaimer"><a href="/stanford-ufldl/archive/wiki/Ufldl_General_disclaimer" title="Ufldl:General disclaimer">Disclaimers</a></li>
	</ul>
</div>
</div>

<!-- Served in 0.120 secs. -->
</body>
</html>
